<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://metal3.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://metal3.io/" rel="alternate" type="text/html" /><updated>2024-05-14T20:11:27-05:00</updated><id>https://metal3.io/feed.xml</id><title type="html">Metal³ - Metal Kubed</title><subtitle>Metal3.io aims to build on baremetal host provisioning technologies to provide a Kubernetes native API for managing bare metal hosts via a provisioning stack that is also running on Kubernetes.</subtitle><entry><title type="html">Metal3 at KubeCon EU 2024</title><link href="https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html" rel="alternate" type="text/html" title="Metal3 at KubeCon EU 2024" /><published>2024-04-10T00:00:00-05:00</published><updated>2024-04-10T00:00:00-05:00</updated><id>https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024</id><content type="html" xml:base="https://metal3.io/blog/2024/04/10/Metal3_at_KubeCon_EU_2024.html"><![CDATA[<p>The Metal3 project was present at KubeCon EU 2024 with multiple maintainers,
contributors and users! For many of us, this was the first time we met in the
physical world, despite working together for years already. This was very
valuable and appreciated by many of us, I am sure. We had time to casually
discuss ideas and proposals, hack together on the
<a href="https://github.com/metal3-io/ironic-standalone-operator">ironic-standalone-operator</a>
and simply get to know each other.</p>

<p><img src="/assets/2024-04-10-Metal3_at_KubeCon_EU_2024/lightningtalk.jpg" alt="Lightning
talk" /></p>

<p><em>Photo by Michael Captain.</em></p>

<p>As a project, we had the opportunity to give an update through a <a href="https://www.youtube.com/watch?v=6QsOQsQZQS8">lightning
talk</a> on Tuesday!</p>

<!-- markdownlint-disable no-inline-html -->
<iframe width="560" height="315" src="https://www.youtube.com/embed/6QsOQsQZQS8?si=bH5w9svPxM1NE8Le" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<!-- markdownlint-enable no-inline-html -->

<p>On Wednesday we continued with a <a href="https://sched.co/1YheY">contribfest session</a>
where we gave an introduction to the project for potential new contributors. We
had prepared a number of good-first-issue’s that people could choose from if
they wanted. Perhaps more important though, was that we had time to answer
questions, discuss use-cases, issues and features with the attendees. The new
<a href="https://book.metal3.io/quick-start">quick-start</a> page was also launched just in
time for the contribfest. It should hopefully make it easier to get started with
the project and we encourage everyone to run through it and report or fix any
issues found.</p>

<p><img src="/assets/2024-04-10-Metal3_at_KubeCon_EU_2024/contribfest.jpg" alt="Contribfest" /></p>

<p><em>Photo from the official CNCF Flickr. More photos
<a href="https://www.flickr.com/photos/143247548@N03/53609847541/in/album-72177720315561784/">here</a>.</em></p>

<p>Finally, just like previous, we had a table in the Project Pavilion. There was a
lot of interest in Metal3, more than last year I would say. Even with five
maintainers working in parallel, we still had a hard time keeping up with the
amount of people stopping by to ask questions! My takeaway from this event is
that we still have work to do on explaining what Metal3 is and how it works. It
is quite uncommon that people know about baseboard management controllers (BMCs)
and this of course makes it harder to grasp what Metal3 is all about. However,
the interest is there, so we just need to get the information out there so that
people can learn! Another takeaway is that Cluster API in general seems to
really take off. Many people that came by our kiosk knew about Cluster API and
were interested in Metal3 because of the integration with have with it.</p>

<p>For those of you who couldn’t attend, I hope this post gives an idea about what
happened at KubeCon related to Metal3. Did you miss the contribfest? Maybe you
would like to contribute but don’t know where to start? Check out the
<a href="https://github.com/issues?page=1&amp;q=archived%3Afalse+user%3Ametal3-io+label%3A%22good+first+issue%22+is%3Aissue+sort%3Acreated-asc+is%3Aopen">good-first-issue’s</a>!
There are still plenty to choose from, and we will keep adding more.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="talk" /><category term="conference" /><category term="kubecon" /><summary type="html"><![CDATA[The Metal3 project was present at KubeCon EU 2024 with multiple maintainers, contributors and users! For many of us, this was the first time we met in the physical world, despite working together for years already. This was very valuable and appreciated by many of us, I am sure. We had time to casually discuss ideas and proposals, hack together on the ironic-standalone-operator and simply get to know each other.]]></summary></entry><entry><title type="html">How to run Metal3 website locally with Jekyll</title><link href="https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html" rel="alternate" type="text/html" title="How to run Metal3 website locally with Jekyll" /><published>2024-01-18T00:00:00-06:00</published><updated>2024-01-18T00:00:00-06:00</updated><id>https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll</id><content type="html" xml:base="https://metal3.io/blog/2024/01/18/How_to_run_Metal3_website_locally_with_Jekyll.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>If you’re a developer or contributor to the Metal3 project, you may need
to run the Metal3 website locally to test changes and ensure everything
looks as expected before deploying them. In this guide, we’ll walk you
through the process of setting up and running Metal3’s website locally
on your machine using Jekyll.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before we begin, make sure you have the following prerequisites
installed on your system:</p>

<ul>
  <li>
    <p>Ruby: Jekyll, the static site generator used by Metal3, is built with
Ruby. Install Ruby and its development tools by running the following
command in your terminal:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">sudo </span>apt <span class="nb">install </span>ruby-full
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="setting-up-metal3s-website">Setting up Metal3’s Website</h2>

<p>Once Ruby is installed, we can proceed to set up Metal3’s website and
its dependencies. Follow these steps:</p>

<ul>
  <li>
    <p>Clone the Metal3 website repository from GitHub. Open your terminal
and navigate to the directory where you want to clone the repository,
then run the following command:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>git clone https://github.com/metal3-io/metal3-io.github.io.git
</code></pre></div>    </div>
  </li>
  <li>
    <p>Change to the cloned directory:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">cd </span>metal3-io.github.io
</code></pre></div>    </div>
  </li>
  <li>
    <p>Install the required gems and dependencies using Bundler. Run the
following command:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>bundle <span class="nb">install</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="running-the-metal3-website-locally">Running the Metal3 Website Locally</h2>

<p>With Metal3’s website and its dependencies installed, you can now start the local
development server to view and test the website. In the terminal, navigate to the
project’s root directory (<code class="language-plaintext highlighter-rouge">metal3-io.github.io</code>) and run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div>

<p>This command tells Jekyll to build the website and start a local server.
Once the server is running, you’ll see output indicating the local
address where the Metal3 website is being served, typically
<a href="http://localhost:4000">http://localhost:4000</a>.</p>

<p>Open your web browser and enter the provided address. Congratulations!
You should now see the Metal3 website running locally, allowing you to
preview your changes and ensure everything is working as expected.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Running Metal3’s website locally using Jekyll is a great way to test
changes and ensure the site functions properly before deploying them. By
following the steps outlined in this guide, you’ve successfully set up
and run Metal3’s website locally. Feel free to explore the Metal3
documentation and contribute to the project further.</p>]]></content><author><name>Salima Rabiu</name></author><category term="metal3" /><category term="baremetal" /><category term="metal3-dev-env" /><category term="documentation" /><category term="development" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 2</title><link href="https://metal3.io/blog/2023/05/17/Scaling_part_2.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 2" /><published>2023-05-17T00:00:00-05:00</published><updated>2023-05-17T00:00:00-05:00</updated><id>https://metal3.io/blog/2023/05/17/Scaling_part_2</id><content type="html" xml:base="https://metal3.io/blog/2023/05/17/Scaling_part_2.html"><![CDATA[<p>In <a href="/blog/2023/05/05/Scaling_part_1.html">part 1</a>, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts.
Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s.</p>

<h2 id="test-setup">Test setup</h2>

<p>The end goal is to have one management cluster where the Cluster API and Metal3 controllers run.
In this cluster we would generate BareMetalHosts and create Clusters, Metal3Clusters, etc to benchmark the controllers.
To give them a realistic test, we also need to fake the workload cluster API’s.
These will run separately in “backing” clusters to avoid interfering with the test (e.g. by using up all the resources in the management cluster).
Here is a diagram that describes the setup:</p>

<p><img src="/assets/2023-05-17-Scaling_part_2/scaling-fake-clusters.drawio.png" alt="diagram of test setup" /></p>

<p>How are we going to fake the workload cluster API’s then?
The most obvious solution is to just run the real deal, i.e. the <code class="language-plaintext highlighter-rouge">kube-apiserver</code>.
This is what would be run in a real workload cluster, together with the other components that make up the Kubernetes control plane.</p>

<p>If you want to follow along and try to set this up yourself, you will need at least the following tools installed:</p>

<ul>
  <li><a href="https://kind.sigs.k8s.io/docs/user/quick-start">kind</a></li>
  <li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl">kubectl</a></li>
  <li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">kubeadm</a></li>
  <li><a href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">clusterctl</a></li>
  <li><a href="https://github.com/openssl/openssl">openssl</a></li>
  <li><a href="https://curl.se/">curl</a></li>
  <li><a href="https://www.gnu.org/software/wget/">wget</a></li>
</ul>

<p>This has been tested with Kubernetes v1.25, kind v0.19 and clusterctl v1.4.2.
All script snippets are assumed to be for the <code class="language-plaintext highlighter-rouge">bash</code> shell.</p>

<h2 id="running-the-kubernetes-api-server">Running the Kubernetes API server</h2>

<p>There are many misconceptions, maybe even superstitions, about the Kubernetes control plane.
The fact is that it is in no way special.
It consists of a few programs that can be run in any way you want: in a container, as a systemd unit or directly executed at the command line.
They can run on a Node or outside of the cluster.
You can even run multiple instances on the same host as long as you avoid port collisions.</p>

<p>For our purposes we basically want to run as little as possible of the control plane components.
We just need the API to be available and possible for us to populate with data that the controllers expect to be there.
In other words, we need the API server and etcd.
The scheduler is not necessary since we won’t run any actual workload (we are just pretending the Nodes are there anyway) and the controller manager would just get in the way when we want to fake resources.
It would, for example, try to update the status of the (fake) Nodes that we want to create.</p>

<p>The API server will need an etcd instance to connect to.
It will also need some TLS configuration, both for connecting to etcd and for handling service accounts.
One simple way to generate the needed certificates is to use kubeadm.
But before we get there we need to think about how the configuration should look like.</p>

<p>For simplicity, we will simply run the API server and etcd in a kind cluster for now.
It would then be easy to run them in some other Kubernetes cluster later if needed.
Let’s create it right away:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="c"># Note: This has been tested with node image</span>
<span class="c"># kindest/node:v1.26.3@sha256:61b92f38dff6ccc29969e7aa154d34e38b89443af1a2c14e6cfbd2df6419c66f</span>
</code></pre></div></div>

<p>To try to cut down on the resources required, we will also use a single multi-tenant etcd instance instead of one per API server.
We can rely on the internal service discovery so the API server can find etcd via an address like <code class="language-plaintext highlighter-rouge">etcd-server.etd-system.svc.cluster.local</code>, instead of using IP addresses.
Finally, we will need an endpoint where the API is exposed to the cluster where the controllers are running, but for now we can focus on just getting it up and running with <code class="language-plaintext highlighter-rouge">127.0.0.1:6443</code> as the endpoint.</p>

<p>Based on the above, we can create a <code class="language-plaintext highlighter-rouge">kubeadm-config.yaml</code> file like this:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubeadm.k8s.io/v1beta3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ClusterConfiguration</span>
<span class="na">apiServer</span><span class="pi">:</span>
  <span class="na">certSANs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">127.0.0.1</span>
<span class="na">clusterName</span><span class="pi">:</span> <span class="s">test</span>
<span class="na">controlPlaneEndpoint</span><span class="pi">:</span> <span class="s">127.0.0.1:6443</span>
<span class="na">etcd</span><span class="pi">:</span>
  <span class="na">local</span><span class="pi">:</span>
    <span class="na">serverCertSANs</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">etcd-server.etcd-system.svc.cluster.local</span>
    <span class="na">peerCertSANs</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">etcd-0.etcd.etcd-system.svc.cluster.local</span>
<span class="na">kubernetesVersion</span><span class="pi">:</span> <span class="s">v1.25.3</span>
<span class="na">certificatesDir</span><span class="pi">:</span> <span class="s">/tmp/test/pki</span>
</code></pre></div></div>

<p>We can now use this to generate some certificates and upload them to the cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Generate CA certificates</span>
kubeadm init phase certs etcd-ca <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs ca <span class="nt">--config</span> kubeadm-config.yaml
<span class="c"># Generate etcd peer and server certificates</span>
kubeadm init phase certs etcd-peer <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs etcd-server <span class="nt">--config</span> kubeadm-config.yaml

<span class="c"># Upload certificates</span>
kubectl create namespace etcd-system
kubectl <span class="nt">-n</span> etcd-system create secret tls test-etcd <span class="nt">--cert</span> /tmp/test/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/test/pki/etcd/ca.key
kubectl <span class="nt">-n</span> etcd-system create secret tls etcd-peer <span class="nt">--cert</span> /tmp/test/pki/etcd/peer.crt <span class="nt">--key</span> /tmp/test/pki/etcd/peer.key
kubectl <span class="nt">-n</span> etcd-system create secret tls etcd-server <span class="nt">--cert</span> /tmp/test/pki/etcd/server.crt <span class="nt">--key</span> /tmp/test/pki/etcd/server.key
</code></pre></div></div>

<h3 id="deploying-a-multi-tenant-etcd-instance">Deploying a multi-tenant etcd instance</h3>

<p>Now it is time to deploy etcd!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd.yaml <span class="se">\</span>
  | <span class="nb">sed</span> <span class="s2">"s/CLUSTER/test/g"</span> | kubectl <span class="nt">-n</span> etcd-system apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> etcd-system <span class="nb">wait </span>sts/etcd <span class="nt">--for</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.availableReplicas}"</span><span class="o">=</span>1
</code></pre></div></div>

<p>As mentioned before, we want to create a <a href="https://etcd.io/docs/v3.5/op-guide/authentication/rbac/">multi-tenant etcd</a> that many API servers can share.
For this reason, we will need to create a root user and enable authentication for etcd:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create root role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add root
<span class="c"># Create root user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add root <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"rootpw"</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role root root
<span class="c"># Enable authentication</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  auth <span class="nb">enable</span>
</code></pre></div></div>

<p>At this point we have a working etcd instance with authentication and TLS enabled.
Each client will need to have an etcd user to interact with this instance so we need to create an etcd user for the API server.
We already created a root user before so this should look familiar.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">## Create etcd tenant</span>
<span class="c"># Create user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add <span class="nb">test</span> <span class="nt">--new-user-password</span><span class="o">=</span><span class="nb">test</span>
<span class="c"># Create role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add <span class="nb">test</span>
<span class="c"># Add read/write permissions for prefix to the role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role grant-permission <span class="nb">test</span> <span class="nt">--prefix</span><span class="o">=</span><span class="nb">true </span>readwrite <span class="s2">"/test/"</span>
<span class="c"># Give the user permissions from the role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role <span class="nb">test test</span>
</code></pre></div></div>

<p>From etcd’s point of view, everything is now ready.
The API server could theoretically use <code class="language-plaintext highlighter-rouge">etcdctl</code> and authenticate with the username and password that we created for it.
However, that is not how the API server works.
It expects to be able to authenticate using client certificates.
Luckily, etcd supports this so we just have to generate the certificates and sign them so that etcd trusts them.
The key thing is to set the common name in the certificate to the name of the user we want to authenticate as.</p>

<p>Since <code class="language-plaintext highlighter-rouge">kubeadm</code> always sets the same common name, we will here use <code class="language-plaintext highlighter-rouge">openssl</code> to generate the client certificates so that we get control over it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Generate etcd client certificate</span>
openssl req <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-subj</span> <span class="s2">"/CN=test"</span> <span class="se">\</span>
 <span class="nt">-keyout</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.key"</span> <span class="nt">-out</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.csr"</span>
openssl x509 <span class="nt">-req</span> <span class="nt">-in</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.csr"</span> <span class="se">\</span>
  <span class="nt">-CA</span> /tmp/test/pki/etcd/ca.crt <span class="nt">-CAkey</span> /tmp/test/pki/etcd/ca.key <span class="nt">-CAcreateserial</span> <span class="se">\</span>
  <span class="nt">-out</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.crt"</span> <span class="nt">-days</span> 365
</code></pre></div></div>

<h3 id="deploying-the-api-server">Deploying the API server</h3>

<p>In order to deploy the API server, we will first need to generate some more certificates.
The client certificates for connecting to etcd are already ready, but it also needs certificates to secure the exposed API itself, and a few other things.
Then we will also need to create secrets from all of these certificates:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubeadm init phase certs ca <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs apiserver <span class="nt">--config</span> kubeadm-config.yaml
kubeadm init phase certs sa <span class="nt">--cert-dir</span> /tmp/test/pki

kubectl create ns workload-api
kubectl <span class="nt">-n</span> workload-api create secret tls test-ca <span class="nt">--cert</span> /tmp/test/pki/ca.crt <span class="nt">--key</span> /tmp/test/pki/ca.key
kubectl <span class="nt">-n</span> workload-api create secret tls test-etcd <span class="nt">--cert</span> /tmp/test/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/test/pki/etcd/ca.key
kubectl <span class="nt">-n</span> workload-api create secret tls <span class="s2">"test-apiserver-etcd-client"</span> <span class="se">\</span>
  <span class="nt">--cert</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.crt"</span> <span class="se">\</span>
  <span class="nt">--key</span> <span class="s2">"/tmp/test/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">-n</span> workload-api create secret tls apiserver <span class="se">\</span>
  <span class="nt">--cert</span> <span class="s2">"/tmp/test/pki/apiserver.crt"</span> <span class="se">\</span>
  <span class="nt">--key</span> <span class="s2">"/tmp/test/pki/apiserver.key"</span>
kubectl <span class="nt">-n</span> workload-api create secret generic test-sa <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span>tls.crt<span class="o">=</span><span class="s2">"/tmp/test/pki/sa.pub"</span> <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span>tls.key<span class="o">=</span><span class="s2">"/tmp/test/pki/sa.key"</span>
</code></pre></div></div>

<p>With all that out of the way, we can finally deploy the API server!
For this we will use a normal Deployment.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Deploy API server</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment.yaml |
  <span class="nb">sed</span> <span class="s2">"s/CLUSTER/test/g"</span> | kubectl <span class="nt">-n</span> workload-api apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> workload-api <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available deploy/test-kube-apiserver
</code></pre></div></div>

<p>Time to check if it worked!
We can use port-forwarding to access the API, but of course we will need some authentication method for it to be useful.
With kubeadm we can generate a kubeconfig based on the certificates we already have.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubeadm kubeconfig user <span class="nt">--client-name</span> kubernetes-admin <span class="nt">--org</span> system:masters <span class="se">\</span>
  <span class="nt">--config</span> kubeadm-config.yaml <span class="o">&gt;</span> kubeconfig.yaml
</code></pre></div></div>

<p>Now open another terminal and set up port-forwarding to the API server:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">-n</span> workload-api port-forward svc/test-kube-apiserver 6443
</code></pre></div></div>

<p>Back in the original terminal, you should now be able to reach the workload API server:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml cluster-info
</code></pre></div></div>

<p>Note that it won’t have any Nodes or Pods running.
It is completely empty since it is running on its own.
There is no kubelet that registered as a Node or applied static manifests, there is no scheduler or controller manager.
Exactly like we want it.</p>

<h2 id="faking-nodes-and-other-resources">Faking Nodes and other resources</h2>

<p>Let’s take a step back and think about what we have done so far.
We have deployed a Kubernetes API server and a multi-tenant etcd instance.
More API servers can be added in the same way, so it is straight forward to scale.
All of it runs in a kind cluster, which means that it is easy to set up and we can switch to any other Kubernetes cluster if needed later.
Through Kubernetes we also get an easy way to access the API servers by using port-forwarding, without exposing all of them separately.</p>

<p>The time has now come to think about what we need to put in the workload cluster API to convince the Cluster API and Metal3 controllers that it is healthy.
First of all they will expect to see Nodes that match the Machines and that they have a provider ID set.
Secondly, they will expect to see healthy control plane Pods.
Finally, they will try to check on the etcd cluster.</p>

<p>The final point is a problem, but we can work around it for now by configuring <a href="https://cluster-api.sigs.k8s.io/tasks/external-etcd.html">external etcd</a>.
It will lead to a different code path for the bootstrap and control plane controllers, but until we have something better it will be a good enough test.</p>

<p>Creating the Nodes and control plane Pods is really easy though.
We are just adding resources and there are no controllers or validating web hooks that can interfere.
Try it out!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create a Node</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml create <span class="nt">-f</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node.yaml
<span class="c"># Check that it worked</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml get nodes
<span class="c"># Maybe label it as part of the control plane?</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml label node fake-node node-role.kubernetes.io/control-plane<span class="o">=</span><span class="s2">""</span>
</code></pre></div></div>

<p>Now add a Pod:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml create <span class="nt">-f</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod.yaml
<span class="c"># Set status on the pods (it is not added when using create/apply).</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span>kubeconfig.yaml <span class="nt">-n</span> kube-system patch pod kube-apiserver-node-name <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
</code></pre></div></div>

<p>You should be able to see something like this:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml get pods <span class="nt">-A</span>
<span class="go">NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE
kube-system   kube-apiserver-node-name   1/1     Running   0          16h
</span><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">--kubeconfig</span> kubeconfig.yaml get nodes
<span class="go">NAME        STATUS   ROLES    AGE   VERSION
</span><span class="gp">fake-node   Ready    &lt;none&gt;</span><span class="w">   </span>16h   v1.25.3
</code></pre></div></div>

<p>Now all we have to do is to ensure that the API returns information that the controllers expect.</p>

<h2 id="hooking-up-the-api-server-to-a-cluster-api-cluster">Hooking up the API server to a Cluster API cluster</h2>

<p>We will now set up a fresh cluster where we can run the Cluster API and Metal3 controllers.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Delete the previous cluster</span>
kind delete cluster
<span class="c"># Create a fresh new cluster</span>
kind create cluster
<span class="c"># Initialize Cluster API with Metal3</span>
clusterctl init <span class="nt">--infrastructure</span> metal3
<span class="c">## Deploy the Bare Metal Opearator</span>
<span class="c"># Create the namespace where it will run</span>
kubectl create ns baremetal-operator-system
<span class="c"># Deploy it in normal mode</span>
kubectl apply <span class="nt">-k</span> https://github.com/metal3-io/baremetal-operator/config/default
<span class="c"># Patch it to run in test mode</span>
kubectl patch <span class="nt">-n</span> baremetal-operator-system deploy baremetal-operator-controller-manager <span class="nt">--type</span><span class="o">=</span>json <span class="se">\</span>
  <span class="nt">-p</span><span class="o">=</span><span class="s1">'[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--test-mode"}]'</span>
</code></pre></div></div>

<p>You should now have a cluster with the Cluster API, Metal3 provider and Bare Metal Operator running.
Next, we will prepare some files that will come in handy later, namely a cluster template, BareMetalHost manifest and Kubeadm configuration file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Download cluster-template</span>
<span class="nv">CLUSTER_TEMPLATE</span><span class="o">=</span>/tmp/cluster-template.yaml
<span class="c"># https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/examples/clusterctl-templates/clusterctl-cluster.yaml</span>
<span class="nv">CLUSTER_TEMPLATE_URL</span><span class="o">=</span><span class="s2">"https://raw.githubusercontent.com/metal3-io/cluster-api-provider-metal3/main/examples/clusterctl-templates/clusterctl-cluster.yaml"</span>
wget <span class="nt">-O</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE_URL</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Save a manifest of a BareMetalHost</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /tmp/test-hosts.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-1-bmc-secret
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-1
spec:
  online: true
  bmc:
    address: libvirt://192.168.122.1:6233/
    credentialsName: worker-1-bmc-secret
  bootMACAddress: "00:60:2F:10:E9:A7"
</span><span class="no">EOF

</span><span class="c"># Save a kubeadm config template</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /tmp/kubeadm-config-template.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs:
    - localhost
    - 127.0.0.1
    - 0.0.0.0
    - HOST
clusterName: test
controlPlaneEndpoint: HOST:6443
etcd:
  local:
    serverCertSANs:
      - etcd-server.etcd-system.svc.cluster.local
    peerCertSANs:
      - etcd-0.etcd.etcd-system.svc.cluster.local
kubernetesVersion: v1.25.3
certificatesDir: /tmp/CLUSTER/pki
</span><span class="no">EOF
</span></code></pre></div></div>

<p>With this we have enough to start creating the workload cluster.
First, we need to set up some certificates.
This should look very familiar from earlier when we created certificates for the Kubernetes API server and etcd.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /tmp/pki/etcd
<span class="nv">CLUSTER</span><span class="o">=</span><span class="s2">"test"</span>
<span class="nv">NAMESPACE</span><span class="o">=</span>etcd-system
<span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="o">=</span><span class="s2">"test-kube-apiserver.</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">.svc.cluster.local"</span>

<span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/NAMESPACE/</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/</span><span class="se">\/</span><span class="s2">CLUSTER//g"</span> <span class="nt">-e</span> <span class="s2">"s/HOST/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/g"</span> <span class="se">\</span>
  /tmp/kubeadm-config-template.yaml <span class="o">&gt;</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>

<span class="c"># Generate CA certificates</span>
kubeadm init phase certs etcd-ca <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs ca <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Generate etcd peer and server certificates</span>
kubeadm init phase certs etcd-peer <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs etcd-server <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
</code></pre></div></div>

<p>Next, we create the namespace, the BareMetalHost and secrets from the certificates:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">CLUSTER</span><span class="o">=</span>test-1
<span class="nv">NAMESPACE</span><span class="o">=</span>test-1
kubectl create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> /tmp/test-hosts.yaml
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">--cert</span> /tmp/pki/ca.crt <span class="nt">--key</span> /tmp/pki/ca.key
</code></pre></div></div>

<p>We are now ready to create the cluster!
We just need a few variables for the template.
The important part here is the <code class="language-plaintext highlighter-rouge">CLUSTER_APIENDPOINT_HOST</code> and <code class="language-plaintext highlighter-rouge">CLUSTER_APIENDPOINT_PORT</code>, since this will be used by the controllers to connect to the workload cluster API.
You should set the IP to the private IP of the test machine or similar.
This way we can use port-forwarding to expose the API on this IP, which the controllers can then reach.
The port just have to be one not in use, and preferably something that is easy to remember and associate with the correct cluster.
For example, cluster 1 gets port 10001, cluster 2 gets 10002, etc.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">export </span><span class="nv">IMAGE_CHECKSUM</span><span class="o">=</span><span class="s2">"97830b21ed272a3d854615beb54cf004"</span>
<span class="nb">export </span><span class="nv">IMAGE_CHECKSUM_TYPE</span><span class="o">=</span><span class="s2">"md5"</span>
<span class="nb">export </span><span class="nv">IMAGE_FORMAT</span><span class="o">=</span><span class="s2">"raw"</span>
<span class="nb">export </span><span class="nv">IMAGE_URL</span><span class="o">=</span><span class="s2">"http://172.22.0.1/images/rhcos-ootpa-latest.qcow2"</span>
<span class="nb">export </span><span class="nv">KUBERNETES_VERSION</span><span class="o">=</span><span class="s2">"v1.25.3"</span>
<span class="nb">export </span><span class="nv">WORKERS_KUBEADM_EXTRA_CONFIG</span><span class="o">=</span><span class="s2">""</span>
<span class="nb">export </span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="o">=</span><span class="s2">"172.17.0.2"</span>
<span class="nb">export </span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="o">=</span><span class="s2">"10001"</span>
<span class="nb">export </span><span class="nv">CTLPLANE_KUBEADM_EXTRA_CONFIG</span><span class="o">=</span><span class="s2">"
    clusterConfiguration:
      controlPlaneEndpoint: </span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">:</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">
      apiServer:
        certSANs:
        - localhost
        - 127.0.0.1
        - 0.0.0.0
        - </span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">
      etcd:
        external:
          endpoints:
            - https://etcd-server:2379
          caFile: /etc/kubernetes/pki/etcd/ca.crt
          certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
          keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key"</span>
</code></pre></div></div>

<p>Create the cluster!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl generate cluster <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--from</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_TEMPLATE</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--target-namespace</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> | kubectl apply <span class="nt">-f</span> -
</code></pre></div></div>

<p>This will give you a cluster and all the templates and other resources that are needed.
However, we will need to fill in for the non-existent hardware and create the workload cluster API server, like we practiced before.
This time it is slightly different, because some of the steps are handled by the Cluster API.
We just need to take care of what would happen on the node, plus the etcd part since we are using external etcd configuration.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/etcd"</span>

<span class="c"># Generate etcd client certificate</span>
openssl req <span class="nt">-newkey</span> rsa:2048 <span class="nt">-nodes</span> <span class="nt">-subj</span> <span class="s2">"/CN=</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
 <span class="nt">-keyout</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span> <span class="nt">-out</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.csr"</span>
openssl x509 <span class="nt">-req</span> <span class="nt">-in</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.csr"</span> <span class="se">\</span>
  <span class="nt">-CA</span> /tmp/pki/etcd/ca.crt <span class="nt">-CAkey</span> /tmp/pki/etcd/ca.key <span class="nt">-CAcreateserial</span> <span class="se">\</span>
  <span class="nt">-out</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">-days</span> 365

<span class="c"># Get the k8s ca certificate and key.</span>
<span class="c"># This is used by kubeadm to generate the api server certificates</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/ca.crt"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">key}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> <span class="o">&gt;</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/ca.key"</span>

<span class="c"># Generate certificates</span>
<span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/NAMESPACE/</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/HOST/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/g"</span> <span class="se">\</span>
  /tmp/kubeadm-config-template.yaml <span class="o">&gt;</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
kubeadm init phase certs apiserver <span class="nt">--config</span> <span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>

<span class="c"># Create secrets</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-apiserver-etcd-client"</span> <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls apiserver <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.key"</span>
</code></pre></div></div>

<p>Now we will need to set up the fake cluster resources.
For this we will create a second kind cluster and set up etcd, just like we did before.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Note: This will create a kubeconfig context named kind-backing-cluster-1,</span>
<span class="c"># i.e. "kind-" is prefixed to the name.</span>
kind create cluster <span class="nt">--name</span> backing-cluster-1

<span class="c"># Setup central etcd</span>
<span class="nv">CLUSTER</span><span class="o">=</span><span class="s2">"test"</span>
<span class="nv">NAMESPACE</span><span class="o">=</span>etcd-system
kubectl create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>

<span class="c"># Upload certificates</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls etcd-peer <span class="nt">--cert</span> /tmp/pki/etcd/peer.crt <span class="nt">--key</span> /tmp/pki/etcd/peer.key
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls etcd-server <span class="nt">--cert</span> /tmp/pki/etcd/server.crt <span class="nt">--key</span> /tmp/pki/etcd/server.key

<span class="c"># Deploy ETCD</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/etcd.yaml <span class="se">\</span>
  | <span class="nb">sed</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> | kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> -
kubectl <span class="nt">-n</span> etcd-system <span class="nb">wait </span>sts/etcd <span class="nt">--for</span><span class="o">=</span><span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.status.availableReplicas}"</span><span class="o">=</span>1

<span class="c"># Create root role</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add root
<span class="c"># Create root user</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add root <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"rootpw"</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role root root
<span class="c"># Enable authentication</span>
kubectl <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  auth <span class="nb">enable</span>
</code></pre></div></div>

<p>Switch the context back to the first cluster with <code class="language-plaintext highlighter-rouge">kubectl config use-context kind-kind</code> so we don’t get confused about which is the main cluster.
We will now need to put all the expected certificates for the fake cluster in the <code class="language-plaintext highlighter-rouge">kind-backing-cluster-1</code> so that they can be used by the API server that we will deploy there.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">CLUSTER</span><span class="o">=</span>test-1
<span class="nv">NAMESPACE</span><span class="o">=</span>test-1
<span class="c"># Setup fake resources for cluster test-1</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 create namespace <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-etcd"</span> <span class="nt">--cert</span> /tmp/pki/etcd/ca.crt <span class="nt">--key</span> /tmp/pki/etcd/ca.key
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-ca"</span> <span class="nt">--cert</span> /tmp/pki/ca.crt <span class="nt">--key</span> /tmp/pki/ca.key
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-apiserver-etcd-client"</span> <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver-etcd-client.key"</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> create secret tls apiserver <span class="nt">--cert</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.crt"</span> <span class="nt">--key</span> <span class="s2">"/tmp/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/pki/apiserver.key"</span>

kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secrets <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">-sa"</span> <span class="nt">-o</span> yaml | kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 create <span class="nt">-f</span> -

<span class="c">## Create etcd tenant</span>
<span class="c"># Create user</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user add <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--new-user-password</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Create role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role add <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Add read/write permissions for prefix to the role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  role grant-permission <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--prefix</span><span class="o">=</span><span class="nb">true </span>readwrite <span class="s2">"/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/"</span>
<span class="c"># Give the user permissions from the role</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> etcd-system <span class="nb">exec </span>etcd-0 <span class="nt">--</span> etcdctl <span class="nt">--user</span> root:rootpw <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.key <span class="nt">--cert</span><span class="o">=</span>/etc/kubernetes/pki/etcd/tls.crt <span class="nt">--cacert</span> /etc/kubernetes/pki/ca/tls.crt <span class="se">\</span>
  user grant-role <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Check that the Metal3Machine is associated with a BareMetalHost.
Deploy the API server.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Deploy API server</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/manifests/v2/kube-apiserver-deployment.yaml |
  <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/CLUSTER/</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">/g"</span> | kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> apply <span class="nt">-f</span> -
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> <span class="nb">wait</span> <span class="nt">--for</span><span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Available deploy/test-kube-apiserver

<span class="c"># Get kubeconfig</span>
clusterctl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get kubeconfig <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> <span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Edit kubeconfig to point to 127.0.0.1:${CLUSTER_APIENDPOINT_PORT}</span>
<span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s2">"s/</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">/127.0.0.1/"</span> <span class="nt">-e</span> <span class="s2">"s/:6443/:</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">/"</span> <span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
<span class="c"># Port forward for accessing the API</span>
kubectl <span class="nt">--context</span><span class="o">=</span>kind-backing-cluster-1 <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> port-forward <span class="se">\</span>
      <span class="nt">--address</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_HOST</span><span class="k">}</span><span class="s2">,127.0.0.1"</span> svc/test-kube-apiserver <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_APIENDPOINT_PORT</span><span class="k">}</span><span class="s2">"</span>:6443 &amp;
<span class="c"># Check that it is working</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> cluster-info
</code></pre></div></div>

<p>Now that we have a working API for the workload cluster, the only remaining thing is to put everything that the controllers expect in it.
This includes adding a Node to match the Machine as well as static pods that Cluster API expects to be there.
Let’s start with the Node!
The Node must have the correct name and a label with the BareMetalHost UID so that the controllers can put the correct provider ID on it.
We have only created 1 BareMetalHost so it is easy to pick the correct one.
The name of the Node should be the same as the Machine, which is also only a single one.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nv">machine</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get machine <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.name}"</span><span class="si">)</span><span class="s2">"</span>
<span class="nv">bmh_uid</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get bmh <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.items[0].metadata.uid}"</span><span class="si">)</span><span class="s2">"</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/fake-node.yaml |
  <span class="nb">sed</span> <span class="nt">-e</span> <span class="s2">"s/fake-node/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> <span class="nt">-e</span> <span class="s2">"s/fake-uuid/</span><span class="k">${</span><span class="nv">bmh_uid</span><span class="k">}</span><span class="s2">/g"</span> | <span class="se">\</span>
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
<span class="c"># Label it as control-plane since this is a control-plane node.</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> label node <span class="s2">"</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> node-role.kubernetes.io/control-plane<span class="o">=</span><span class="s2">""</span>
<span class="c"># Upload kubeadm config to configmap. This will mark the KCP as initialized.</span>
kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system create cm kubeadm-config <span class="se">\</span>
  <span class="nt">--from-file</span><span class="o">=</span><span class="nv">ClusterConfiguration</span><span class="o">=</span><span class="s2">"/tmp/kubeadm-config-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span>
</code></pre></div></div>

<p>This should be enough to make the Machines healthy!
You should be able to see something similar to this:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>clusterctl <span class="nt">-n</span> test-1 describe cluster test-1
<span class="go">NAME                                            READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/test-1                                  True                     46s
├─ClusterInfrastructure - Metal3Cluster/test-1  True                     114m
└─ControlPlane - KubeadmControlPlane/test-1     True                     46s
  └─Machine/test-1-f2nw2                        True                     47s
</span></code></pre></div></div>

<p>However, if you check the KubeadmControlPlane more carefully, you will notice that it is still complaining about control plane components.
This is because we have not created the static pods yet, and it is also unable to check the certificate expiration date for the Machine.
Let’s fix it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Add static pods to make kubeadm control plane manager happy</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod.yaml |
  <span class="nb">sed</span> <span class="s2">"s/node-name/</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">/g"</span> |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> create <span class="nt">-f</span> -
<span class="c"># Set status on the pods (it is not added when using create/apply).</span>
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-apiserver-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-apiserver-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-controller-manager-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-controller-manager-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin
curl <span class="nt">-L</span> https://github.com/Nordix/metal3-clusterapi-docs/raw/main/metal3-scaling-experiments/kube-scheduler-pod-status.yaml |
  kubectl <span class="nt">--kubeconfig</span><span class="o">=</span><span class="s2">"/tmp/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER</span><span class="k">}</span><span class="s2">.yaml"</span> <span class="nt">-n</span> kube-system patch pod <span class="s2">"kube-scheduler-</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--subresource</span><span class="o">=</span>status <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin

<span class="c"># Add certificate expiry annotations to make kubeadm control plane manager happy</span>
<span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="o">=</span><span class="s2">"machine.cluster.x-k8s.io/certificates-expiry"</span>
<span class="nv">EXPIRY_TEXT</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> get secret apiserver <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{.data.tls</span><span class="se">\.</span><span class="s2">crt}"</span> | <span class="nb">base64</span> <span class="nt">-d</span> | openssl x509 <span class="nt">-enddate</span> <span class="nt">-noout</span> | <span class="nb">cut</span> <span class="nt">-d</span><span class="o">=</span> <span class="nt">-f</span> 2<span class="si">)</span><span class="s2">"</span>
<span class="nv">EXPIRY</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">date</span> <span class="nt">--date</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">EXPIRY_TEXT</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--iso-8601</span><span class="o">=</span>seconds<span class="si">)</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> annotate machine <span class="s2">"</span><span class="k">${</span><span class="nv">machine</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="k">}</span><span class="s2">=</span><span class="k">${</span><span class="nv">EXPIRY</span><span class="k">}</span><span class="s2">"</span>
kubectl <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NAMESPACE</span><span class="k">}</span><span class="s2">"</span> annotate kubeadmconfig <span class="nt">--all</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CERT_EXPIRY_ANNOTATION</span><span class="k">}</span><span class="s2">=</span><span class="k">${</span><span class="nv">EXPIRY</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Now we finally have a completely healthy cluster as far as the controllers are concerned.</p>

<h2 id="conclusions-and-summary">Conclusions and summary</h2>

<p>We now have all the tools necessary to start experimenting.</p>

<ul>
  <li>With the BareMetal Operator running in test mode, we can skip Ironic and still work with BareMetalHosts that act like normal.</li>
  <li>We can set up separate “backing” clusters where we run etcd and multiple API servers to fake the workload cluster API’s.</li>
  <li>Fake Nodes and Pods can be easily added to the workload cluster API’s, and configured as we want.</li>
  <li>The workload cluster API’s can be exposed to the controllers in the test cluster using port-forwarding.</li>
</ul>

<p>In this post we have not automated all of this, but if you want to see a scripted setup, take a look at <a href="https://github.com/Nordix/metal3-clusterapi-docs/tree/main/metal3-scaling-experiments">this</a>.
It is what we used to scale to 1000 clusters.
Just remember that it may need some tweaking for your specific environment if you want to try it out!</p>

<p>Specifically we used 10 “backing” clusters, i.e. 10 separate cloud VMs with kind clusters where we run etcd and the workload cluster API’s.
Each one would hold 100 API servers.
The test cluster was on its own separate VM also running a kind cluster with all the controllers and all the Cluster objects, etc.</p>

<p>In the next and final blog post of this series we will take a look at the results of all this.
What issues did we run into along the way?
How did we fix or work around them?
We will also take a look at what is going on in the community related to this and discuss potential future work in the area.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[In part 1, we introduced the Bare Metal Operator test mode and saw how it can be used to play with BareMetalHosts without Ironic and without any actual hosts. Now we will take a look at the other end of the stack and how we can fake the workload cluster API’s.]]></summary></entry><entry><title type="html">Scaling to 1000 clusters - Part 1</title><link href="https://metal3.io/blog/2023/05/05/Scaling_part_1.html" rel="alternate" type="text/html" title="Scaling to 1000 clusters - Part 1" /><published>2023-05-05T00:00:00-05:00</published><updated>2023-05-05T00:00:00-05:00</updated><id>https://metal3.io/blog/2023/05/05/Scaling_part_1</id><content type="html" xml:base="https://metal3.io/blog/2023/05/05/Scaling_part_1.html"><![CDATA[<p>We want to ensure that Metal3 can scale to thousands of nodes and clusters.
However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project.
So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers.
In this first part we will take a look at the Bare Metal Operator and the <a href="https://github.com/metal3-io/baremetal-operator/blob/b76dde223937009cebb9da85e6f1793a544675e6/docs/dev-setup.md?plain=1#L62">test mode</a> it offers.
The next part will be about how to fake the Kubernetes API of the workload clusters.
In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling!</p>

<h2 id="some-background-on-how-to-fool-the-controllers">Some background on how to fool the controllers</h2>

<p>With the full Metal3 stack, from Ironic to Cluster API, we have the following controllers that operate on Kubernetes APIs:</p>

<ul>
  <li>Cluster API Kubeadm control plane controller</li>
  <li>Cluster API Kubeadm bootstrap controller</li>
  <li>Cluster API controller</li>
  <li>Cluster API provider for Metal3 controller</li>
  <li>IP address manager controller</li>
  <li>Bare Metal Operator controller</li>
</ul>

<p>We will first focus on the controllers that interact with Nodes, Machines, Metal3Machines and BareMetalHosts, i.e. objects related to actual physical machines that we need to fake.
In other words, we are skipping the IP address manager for now.</p>

<p>What do these controllers care about really?
What do we need to do to fool them?
At the Cluster API level, the controllers just care about the Kubernetes resources in the management cluster (e.g. Clusters and Machines) and some resources in the workload cluster (e.g. Nodes and the etcd Pods).
The controllers will try to connect to the workload clusters in order to check the status of the resources there, so if there is no real workload cluster, this is something we will need to fake if we want to fool the controllers.
When it comes to Cluster API provider for Metal3, it connects the abstract high level objects with the BareMetalHosts, so here we will need to make the BareMetalHosts to behave realistically in order to provide a good test.</p>

<p>This is where the Bare Metal Operator test mode comes in.
If we can fake the workload cluster API and the BareMetalHosts, then all the Cluster API controllers and the Metal3 provider will get a realistic test that we can use when working on scalability.</p>

<h2 id="bare-metal-operator-test-mode">Bare Metal Operator test mode</h2>

<p>The Bare Metal Operator has a test mode, in which it doesn’t talk to Ironic.
Instead it just pretends that everything is fine and all actions succeed.
In this mode the BareMetalHosts will move through the state diagram just like they normally would (but quite a bit faster).
To enable it, all you have to do is add the <code class="language-plaintext highlighter-rouge">-test-mode</code> flag when running the Bare Metal Operator controller.
For convenience there is also a make target (<code class="language-plaintext highlighter-rouge">make run-test-mode</code>) that will run the Bare Metal Operator directly on the host in test mode.</p>

<p>Here is an example of how to use it.
You will need kind and kubectl installed for this to work, but you don’t need the Bare Metal Operator repository cloned.</p>

<ol>
  <li>
    <p>Create a kind cluster and deploy cert-manager (needed for web hook certificates):</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kind create cluster
<span class="c"># Install cert-manager</span>
kubectl apply <span class="nt">-f</span> https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml
</code></pre></div>    </div>
  </li>
  <li>
    <p>Deploy the Bare Metal Operator in test mode:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c"># Create the namespace where it will run</span>
kubectl create ns baremetal-operator-system
<span class="c"># Deploy it in normal mode</span>
kubectl apply <span class="nt">-k</span> https://github.com/metal3-io/baremetal-operator/config/default
<span class="c"># Patch it to run in test mode</span>
kubectl patch <span class="nt">-n</span> baremetal-operator-system deploy baremetal-operator-controller-manager <span class="nt">--type</span><span class="o">=</span>json <span class="se">\</span>
  <span class="nt">-p</span><span class="o">=</span><span class="s1">'[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--test-mode"}]'</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>In a separate terminal, create a BareMetalHost from the example manifests:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl apply <span class="nt">-f</span> https://github.com/metal3-io/baremetal-operator/raw/main/examples/example-host.yaml
</code></pre></div>    </div>
  </li>
</ol>

<p>After applying the BareMetalHost, it will quickly go through <code class="language-plaintext highlighter-rouge">registering</code> and become <code class="language-plaintext highlighter-rouge">available</code>.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE         CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   registering              true             2s
</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE       CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   available              true             6s
</span></code></pre></div></div>

<p>We can now provision the BareMetalHost, turn it off, deprovision, etc.
Just like normal, except that the machine doesn’t exist.
Let’s try provisioning it!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>kubectl patch bmh example-baremetalhost <span class="nt">--type</span><span class="o">=</span>merge <span class="nt">--patch-file</span><span class="o">=</span>/dev/stdin <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh">
spec:
  image:
    url: "http://example.com/totally-fake-image.vmdk"
    checksum: "made-up-checksum"
    format: vmdk
</span><span class="no">EOF
</span></code></pre></div></div>

<p>You will see it go through <code class="language-plaintext highlighter-rouge">provisioning</code> and end up in <code class="language-plaintext highlighter-rouge">provisioned</code> state:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE          CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   provisioning              true             7m20s

</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME                    STATE         CONSUMER   ONLINE   ERROR   AGE
example-baremetalhost   provisioned              true             7m22s
</span></code></pre></div></div>

<h2 id="wrapping-up">Wrapping up</h2>

<p>With Bare Metal Operator in test mode, we have the foundation for starting our scalability journey.
We can easily create BareMetalHost objects and they behave similar to what they would in a real scenario.
A simple bash script will at this point allow us to create as many BareMetalHosts as we would like.
To wrap things up, we will now do just that: put together a script and try generating a few BareMetalHosts.</p>

<p>The script will do the same thing we did before when creating the example BareMetalHost, but it will also give them different names so we don’t get naming collisions.
Here it is:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c">#!/usr/bin/env bash</span>

<span class="nb">set</span> <span class="nt">-eu</span>

create_bmhs<span class="o">()</span> <span class="o">{</span>
  <span class="nv">n</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">}</span><span class="s2">"</span>
  <span class="k">for</span> <span class="o">((</span> i <span class="o">=</span> 1<span class="p">;</span> i &lt;<span class="o">=</span> n<span class="p">;</span> ++i <span class="o">))</span><span class="p">;</span> <span class="k">do
    </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-</span><span class="nv">$i</span><span class="sh">-bmc-secret
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-</span><span class="nv">$i</span><span class="sh">
spec:
  online: true
  bmc:
    address: libvirt://192.168.122.</span><span class="nv">$i</span><span class="sh">:6233/
    credentialsName: worker-</span><span class="nv">$i</span><span class="sh">-bmc-secret
  bootMACAddress: "</span><span class="si">$(</span><span class="nb">printf</span> <span class="s1">'00:60:2F:%02X:%02X:%02X\n'</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span> <span class="k">$((</span>RANDOM%256<span class="k">))</span><span class="si">)</span><span class="sh">"
</span><span class="no">EOF
</span>  <span class="k">done</span>
<span class="o">}</span>

<span class="nv">NUM</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">:-</span><span class="nv">10</span><span class="k">}</span><span class="s2">"</span>

create_bmhs <span class="s2">"</span><span class="k">${</span><span class="nv">NUM</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Save it as <code class="language-plaintext highlighter-rouge">produce-available-hosts.sh</code> and try it out:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>./produce-available-hosts.sh 10 | kubectl apply <span class="nt">-f</span> -
<span class="go">secret/worker-1-bmc-secret created
baremetalhost.metal3.io/worker-1 created
secret/worker-2-bmc-secret created
baremetalhost.metal3.io/worker-2 created
secret/worker-3-bmc-secret created
baremetalhost.metal3.io/worker-3 created
secret/worker-4-bmc-secret created
baremetalhost.metal3.io/worker-4 created
secret/worker-5-bmc-secret created
baremetalhost.metal3.io/worker-5 created
secret/worker-6-bmc-secret created
baremetalhost.metal3.io/worker-6 created
secret/worker-7-bmc-secret created
baremetalhost.metal3.io/worker-7 created
secret/worker-8-bmc-secret created
baremetalhost.metal3.io/worker-8 created
secret/worker-9-bmc-secret created
baremetalhost.metal3.io/worker-9 created
secret/worker-10-bmc-secret created
baremetalhost.metal3.io/worker-10 created
</span><span class="gp">$</span><span class="w"> </span>kubectl get bmh
<span class="go">NAME        STATE         CONSUMER   ONLINE   ERROR   AGE
worker-1    registering              true             2s
worker-10   available                true             2s
worker-2    available                true             2s
worker-3    available                true             2s
worker-4    available                true             2s
worker-5    available                true             2s
worker-6    registering              true             2s
worker-7    available                true             2s
worker-8    available                true             2s
worker-9    available                true             2s
</span></code></pre></div></div>

<p>With this we conclude the first part of the scaling series.
In the next post, we will take a look at how to fake the other end of the stack: the workload cluster API.</p>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="edge" /><summary type="html"><![CDATA[We want to ensure that Metal3 can scale to thousands of nodes and clusters. However, running tests with thousands of real servers is expensive and we don’t have access to any such large environment in the project. So instead we have been focusing on faking the hardware while trying to keep things as realistic as possible for the controllers. In this first part we will take a look at the Bare Metal Operator and the test mode it offers. The next part will be about how to fake the Kubernetes API of the workload clusters. In the final post we will take a look at the issues we ran into and what is being done in the community to address them so that we can keep scaling!]]></summary></entry><entry><title type="html">One cluster - multiple providers</title><link href="https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers.html" rel="alternate" type="text/html" title="One cluster - multiple providers" /><published>2022-07-08T00:00:00-05:00</published><updated>2022-07-08T00:00:00-05:00</updated><id>https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers</id><content type="html" xml:base="https://metal3.io/blog/2022/07/08/One_cluster_multiple_providers.html"><![CDATA[<p>Running on bare metal has both benefits and drawbacks. You can get the
best performance possible out of the hardware, but it can also be quite
expensive and maybe not necessary for <em>all</em> workloads. Perhaps a hybrid
cluster could give you the best of both? Raw power for the workload that
needs it, and cheap virtualized commodity for the rest. This blog post
will show how to set up a cluster like this using the Cluster API backed
by the Metal3 and BYOH providers.</p>

<h2 id="the-problem">The problem</h2>

<p>Imagine that you have some bare metal servers that you want to use for
some specific workload. Maybe the workload benefits from the specific
hardware or there are some requirements that make it necessary to run it
there. The rest of the organization already uses Kubernetes and the
cluster API everywhere so of course you want the same for this as well.
Perfect, grab Metal³ and start working!</p>

<p>But hold on, this would mean that you use some of the servers for
running the Kubernetes control plane and possibly all the cluster API
controllers. If there are enough servers this is probably not an issue,
but do you really want to “waste” these servers on such generic
workloads that could be running anywhere? This can become especially
painful if you need multiple control plane nodes. Each server is
probably powerful enough to run all the control planes and controllers,
but it would be a single point of failure…</p>

<p>What if there was a way to use a different cluster API infrastructure
provider for some nodes? For example, use the Openstack infrastructure
provider for the control plane and Metal³ for the workers. Let’s do an
experiment!</p>

<h2 id="setting-up-the-experiment-environment">Setting up the experiment environment</h2>

<p>This blog post will use the <a href="https://github.com/vmware-tanzu/cluster-api-provider-bringyourownhost">Bring your own
host</a>
(BYOH) provider together with Metal³ as a proof of concept to show what
is currently possible.</p>

<p>The BYOH provider was chosen as the second provider for two reasons:</p>

<ol>
  <li>Due to its design (you provision the host yourself), it is very easy
to adapt it to the test (e.g. use a VM in the same network that the
metal3-dev-env uses).</li>
  <li>It is one of the providers that is known to work when combining
multiple providers for a single cluster.</li>
</ol>

<p>We will be using the
<a href="https://github.com/metal3-io/metal3-dev-env">metal3-dev-env</a> on Ubuntu
as a starting point for this experiment. Note that it makes substantial
changes to the machine where it is running, so you may want to use a
dedicated lab machine instead of your laptop for this. If you have not
done so already, clone it and run <code class="language-plaintext highlighter-rouge">make</code>. This should give you a
management cluster with the Metal³ provider installed and two
BareMetalHosts ready for provisioning.</p>

<p>The next step is to add the BYOH provider and a ByoHost.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl init <span class="nt">--infrastructure</span> byoh
</code></pre></div></div>

<p>For the ByoHost we will use Vagrant.
You can install it with <code class="language-plaintext highlighter-rouge">sudo apt install vagrant</code>.
Then copy the Vagrantfile below to a new folder and run <code class="language-plaintext highlighter-rouge">vagrant up</code>.</p>

<pre><code class="language-Vagrantfile"># -*- mode: ruby -*-
hosts = {
    "control-plane1" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.10"},
    # "control-plane2" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.11"},
    # "control-plane3" =&gt; { "memory" =&gt; 2048, "ip" =&gt; "192.168.10.12"},
}


Vagrant.configure("2") do |config|
    # Choose which box you want below
    config.vm.box = "generic/ubuntu2004"
    config.vm.synced_folder ".", "/vagrant", disabled: true
    config.vm.provider :libvirt do |libvirt|
      # QEMU system connection is required for private network configuration
      libvirt.qemu_use_session = false
    end


    # Loop over all machine names
    hosts.each_key do |host|
        config.vm.define host, primary: host == hosts.keys.first do |node|
            node.vm.hostname = host
            node.vm.network :private_network, ip: hosts[host]["ip"],
              libvirt__forward_mode: "route"
            node.vm.provider :libvirt do |lv|
                lv.memory = hosts[host]["memory"]
                lv.cpus = 2
            end
        end
    end
end
</code></pre>

<p>Vagrant should now have created a new VM to use as a ByoHost. Now we
just need to run the BYOH agent in the VM to make it register as a
ByoHost in the management cluster. The BYOH agent needs a kubeconfig
file to do this, so we start by copying it to the VM:</p>

<!-- markdownlint-disable MD013 -->

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nb">cp</span> ~/.kube/config ~/.kube/management-cluster.conf
<span class="c"># Ensure that the correct IP is used (not localhost)</span>
<span class="nb">export </span><span class="nv">KIND_IP</span><span class="o">=</span><span class="si">$(</span>docker inspect <span class="nt">-f</span> <span class="s1">'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'</span> kind-control-plane<span class="si">)</span>
<span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/    server\:.*/    server\: https\:\/\/'</span><span class="s2">"</span><span class="nv">$KIND_IP</span><span class="s2">"</span><span class="s1">'\:6443/g'</span> ~/.kube/management-cluster.conf
scp <span class="nt">-i</span> .vagrant/machines/control-plane1/libvirt/private_key <span class="se">\</span>
  /home/ubuntu/.kube/management-cluster.conf vagrant@192.168.10.10:management-cluster.conf

</code></pre></div></div>

<!-- markdownlint-enable MD013 -->

<p>Next, install the prerequisites and host agent in the VM and run it.</p>

<!-- markdownlint-enable MD013 -->

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>vagrant ssh
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> socat ebtables ethtool conntrack
wget https://github.com/vmware-tanzu/cluster-api-provider-bringyourownhost/releases/download/v0.2.0/byoh-hostagent-linux-amd64
<span class="nb">mv </span>byoh-hostagent-linux-amd64 byoh-hostagent
<span class="nb">chmod</span> +x byoh-hostagent
<span class="nb">sudo</span> ./byoh-hostagent <span class="nt">--namespace</span> metal3 <span class="nt">--kubeconfig</span> management-cluster.conf
</code></pre></div></div>

<!-- markdownlint-disable MD013 -->

<p>You should now have a management cluster with both the Metal³ and BYOH
providers installed, as well as two BareMetalHosts and one ByoHost.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl <span class="nt">-n</span> metal3 get baremetalhosts,byohosts
<span class="go">NAME                             STATE       CONSUMER   ONLINE   ERROR   AGE
baremetalhost.metal3.io/node-0   available              true             18m
baremetalhost.metal3.io/node-1   available              true             18m


NAME                                                     AGE
byohost.infrastructure.cluster.x-k8s.io/control-plane1   73s
</span></code></pre></div></div>

<h2 id="creating-a-multi-provider-cluster">Creating a multi-provider cluster</h2>

<p>The trick is to create both a Metal3Cluster and a ByoCluster that are
owned by one common Cluster. We will use the ByoCluster for the control
plane in this case. First the Cluster:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Cluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">cni</span><span class="pi">:</span> <span class="s">mixed-cluster-crs-0</span>
    <span class="na">crs</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterNetwork</span><span class="pi">:</span>
    <span class="na">pods</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">192.168.0.0/16</span>
    <span class="na">serviceDomain</span><span class="pi">:</span> <span class="s">cluster.local</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">10.128.0.0/12</span>
  <span class="na">controlPlaneRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1beta1</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
  <span class="na">infrastructureRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">ByoCluster</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
</code></pre></div></div>

<p>Add the rest of the BYOH manifests to get a control plane.
The code is collapsed here for easier reading.
Please click on the line below to expand it.</p>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>KubeadmControlPlane, ByoCluster and ByoMachineTemplate</summary>
  <!-- Enable markdown parsing of the content. -->
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">nodepool</span><span class="pi">:</span> <span class="s">pool0</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">kubeadmConfigSpec</span><span class="pi">:</span>
    <span class="na">clusterConfiguration</span><span class="pi">:</span>
      <span class="na">apiServer</span><span class="pi">:</span>
        <span class="na">certSANs</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">localhost</span>
          <span class="pi">-</span> <span class="s">127.0.0.1</span>
          <span class="pi">-</span> <span class="s">0.0.0.0</span>
          <span class="pi">-</span> <span class="s">host.docker.internal</span>
      <span class="na">controllerManager</span><span class="pi">:</span>
        <span class="na">extraArgs</span><span class="pi">:</span>
          <span class="na">enable-hostpath-provisioner</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
    <span class="na">files</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="s">apiVersion: v1</span>
          <span class="s">kind: Pod</span>
          <span class="s">metadata:</span>
            <span class="s">creationTimestamp: null</span>
            <span class="s">name: kube-vip</span>
            <span class="s">namespace: kube-system</span>
          <span class="s">spec:</span>
            <span class="s">containers:</span>
            <span class="s">- args:</span>
              <span class="s">- start</span>
              <span class="s">env:</span>
              <span class="s">- name: vip_arp</span>
                <span class="s">value: "true"</span>
              <span class="s">- name: vip_leaderelection</span>
                <span class="s">value: "true"</span>
              <span class="s">- name: vip_address</span>
                <span class="s">value: 192.168.10.20</span>
              <span class="s">- name: vip_interface</span>
                <span class="s">value: {{ .DefaultNetworkInterfaceName }}</span>
              <span class="s">- name: vip_leaseduration</span>
                <span class="s">value: "15"</span>
              <span class="s">- name: vip_renewdeadline</span>
                <span class="s">value: "10"</span>
              <span class="s">- name: vip_retryperiod</span>
                <span class="s">value: "2"</span>
              <span class="s">image: ghcr.io/kube-vip/kube-vip:v0.3.5</span>
              <span class="s">imagePullPolicy: IfNotPresent</span>
              <span class="s">name: kube-vip</span>
              <span class="s">resources: {}</span>
              <span class="s">securityContext:</span>
                <span class="s">capabilities:</span>
                  <span class="s">add:</span>
                  <span class="s">- NET_ADMIN</span>
                  <span class="s">- SYS_TIME</span>
              <span class="s">volumeMounts:</span>
              <span class="s">- mountPath: /etc/kubernetes/admin.conf</span>
                <span class="s">name: kubeconfig</span>
            <span class="s">hostNetwork: true</span>
            <span class="s">volumes:</span>
            <span class="s">- hostPath:</span>
                <span class="s">path: /etc/kubernetes/admin.conf</span>
                <span class="s">type: FileOrCreate</span>
              <span class="s">name: kubeconfig</span>
          <span class="s">status: {}</span>
        <span class="na">owner</span><span class="pi">:</span> <span class="s">root:root</span>
        <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/kubernetes/manifests/kube-vip.yaml</span>
    <span class="na">initConfiguration</span><span class="pi">:</span>
      <span class="na">nodeRegistration</span><span class="pi">:</span>
        <span class="na">criSocket</span><span class="pi">:</span> <span class="s">/var/run/containerd/containerd.sock</span>
        <span class="na">ignorePreflightErrors</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">Swap</span>
          <span class="pi">-</span> <span class="s">DirAvailable--etc-kubernetes-manifests</span>
          <span class="pi">-</span> <span class="s">FileAvailable--etc-kubernetes-kubelet.conf</span>
    <span class="na">joinConfiguration</span><span class="pi">:</span>
      <span class="na">nodeRegistration</span><span class="pi">:</span>
        <span class="na">criSocket</span><span class="pi">:</span> <span class="s">/var/run/containerd/containerd.sock</span>
        <span class="na">ignorePreflightErrors</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">Swap</span>
          <span class="pi">-</span> <span class="s">DirAvailable--etc-kubernetes-manifests</span>
          <span class="pi">-</span> <span class="s">FileAvailable--etc-kubernetes-kubelet.conf</span>
  <span class="na">machineTemplate</span><span class="pi">:</span>
    <span class="na">infrastructureRef</span><span class="pi">:</span>
      <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
      <span class="na">kind</span><span class="pi">:</span> <span class="s">ByoMachineTemplate</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">version</span><span class="pi">:</span> <span class="s">v1.23.5</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ByoCluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">bundleLookupBaseRegistry</span><span class="pi">:</span> <span class="s">projects.registry.vmware.com/cluster_api_provider_bringyourownhost</span>
  <span class="na">bundleLookupTag</span><span class="pi">:</span> <span class="s">v1.23.5</span>
  <span class="na">controlPlaneEndpoint</span><span class="pi">:</span>
    <span class="na">host</span><span class="pi">:</span> <span class="s">192.168.10.20</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">6443</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ByoMachineTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster-control-plane</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span> <span class="pi">{}</span>

</code></pre></div>    </div>

  </div>
</details>

<p>So far this is a “normal” Cluster backed by the BYOH provider. But now
it is time to do something different. Instead of adding more ByoHosts as
workers, we will add a Metal3Cluster and MachineDeployment backed by
BareMetalHosts! Note that the <code class="language-plaintext highlighter-rouge">controlPlaneEndpoint</code> of the
Metal3Cluster must point to the same endpoint that the ByoCluster is
using.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3Cluster</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">controlPlaneEndpoint</span><span class="pi">:</span>
    <span class="na">host</span><span class="pi">:</span> <span class="s">192.168.10.20</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">6443</span>
  <span class="na">noCloudProvider</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>IPPools</summary>
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">namePrefix</span><span class="pi">:</span> <span class="s">test1-prov</span>
  <span class="na">pools</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">end</span><span class="pi">:</span> <span class="s">172.22.0.200</span>
      <span class="na">start</span><span class="pi">:</span> <span class="s">172.22.0.100</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.111.1</span>
  <span class="na">namePrefix</span><span class="pi">:</span> <span class="s">test1-bmv4</span>
  <span class="na">pools</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">end</span><span class="pi">:</span> <span class="s">192.168.111.200</span>
      <span class="na">start</span><span class="pi">:</span> <span class="s">192.168.111.100</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
</code></pre></div>    </div>

  </div>
</details>

<p>These manifests are quite large but they are just the same as would be
used by the metal3-dev-env with some name changes here and there. The
key thing to note is that all references to a Cluster are to the one we
defined above. Here is the MachineDeployment:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">MachineDeployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
    <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
      <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
        <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">bootstrap</span><span class="pi">:</span>
        <span class="na">configRef</span><span class="pi">:</span>
          <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">bootstrap.cluster.x-k8s.io/v1beta1</span>
          <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmConfigTemplate</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
      <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
      <span class="na">infrastructureRef</span><span class="pi">:</span>
        <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
        <span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
      <span class="na">nodeDrainTimeout</span><span class="pi">:</span> <span class="s">0s</span>
      <span class="na">version</span><span class="pi">:</span> <span class="s">v1.23.5</span>
</code></pre></div></div>

<p>Finally, we add the Metal3MachineTemplate, Metal3DataTemplate and
KubeadmConfigTemplate. Here you may want to add your public ssh key in
the KubeadmConfigTemplate (the last few lines).</p>

<!-- markdownlint-disable MD033 -->

<details>
  <summary>Metal3MachineTemplate, Metal3DataTemplate and KubeadmConfigTemplate</summary>
  <!-- Enable markdown parsing of the content. -->
  <div>

    <!-- markdownlint-enable MD033 -->

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">dataTemplate</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers-template</span>
      <span class="na">image</span><span class="pi">:</span>
        <span class="na">checksum</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/UBUNTU_22.04_NODE_IMAGE_K8S_v1.23.5-raw.img.md5sum</span>
        <span class="na">checksumType</span><span class="pi">:</span> <span class="s">md5</span>
        <span class="na">format</span><span class="pi">:</span> <span class="s">raw</span>
        <span class="na">url</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/UBUNTU_22.04_NODE_IMAGE_K8S_v1.23.5-raw.img</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3DataTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers-template</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">mixed-cluster</span>
  <span class="na">metaData</span><span class="pi">:</span>
    <span class="na">ipAddressesFromIPPool</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">provisioningIP</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
    <span class="na">objectNames</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">name</span>
        <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local-hostname</span>
        <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">local_hostname</span>
        <span class="na">object</span><span class="pi">:</span> <span class="s">machine</span>
    <span class="na">prefixesFromIPPool</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">provisioningCIDR</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">provisioning-pool</span>
  <span class="na">networkData</span><span class="pi">:</span>
    <span class="na">links</span><span class="pi">:</span>
      <span class="na">ethernets</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">enp1s0</span>
          <span class="na">macAddress</span><span class="pi">:</span>
            <span class="na">fromHostInterface</span><span class="pi">:</span> <span class="s">enp1s0</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s">phy</span>
        <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">enp2s0</span>
          <span class="na">macAddress</span><span class="pi">:</span>
            <span class="na">fromHostInterface</span><span class="pi">:</span> <span class="s">enp2s0</span>
          <span class="na">type</span><span class="pi">:</span> <span class="s">phy</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="na">ipv4</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">baremetalv4</span>
          <span class="na">ipAddressFromIPPool</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
          <span class="na">link</span><span class="pi">:</span> <span class="s">enp2s0</span>
          <span class="na">routes</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">gateway</span><span class="pi">:</span>
                <span class="na">fromIPPool</span><span class="pi">:</span> <span class="s">baremetalv4-pool</span>
              <span class="na">network</span><span class="pi">:</span> <span class="s">0.0.0.0</span>
              <span class="na">prefix</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">dns</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">8.8.8.8</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">bootstrap.cluster.x-k8s.io/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmConfigTemplate</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">test1-workers</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">files</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">network:</span>
              <span class="s">version: 2</span>
              <span class="s">renderer: networkd</span>
              <span class="s">bridges:</span>
                <span class="s">ironicendpoint:</span>
                  <span class="s">interfaces: [enp1s0]</span>
                  <span class="s">addresses:</span>
                  <span class="s">- {{ ds.meta_data.provisioningIP }}/{{ ds.meta_data.provisioningCIDR }}</span>
          <span class="na">owner</span><span class="pi">:</span> <span class="s">root:root</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/netplan/52-ironicendpoint.yaml</span>
          <span class="na">permissions</span><span class="pi">:</span> <span class="s2">"</span><span class="s">0644"</span>
        <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
            <span class="s">[registries.search]</span>
            <span class="s">registries = ['docker.io']</span>


            <span class="s">[registries.insecure]</span>
            <span class="s">registries = ['192.168.111.1:5000']</span>
          <span class="na">path</span><span class="pi">:</span> <span class="s">/etc/containers/registries.conf</span>
      <span class="na">joinConfiguration</span><span class="pi">:</span>
        <span class="na">nodeRegistration</span><span class="pi">:</span>
          <span class="na">kubeletExtraArgs</span><span class="pi">:</span>
            <span class="na">cgroup-driver</span><span class="pi">:</span> <span class="s">systemd</span>
            <span class="na">container-runtime</span><span class="pi">:</span> <span class="s">remote</span>
            <span class="na">container-runtime-endpoint</span><span class="pi">:</span> <span class="s">unix:///var/run/crio/crio.sock</span>
            <span class="na">feature-gates</span><span class="pi">:</span> <span class="s">AllAlpha=false</span>
            <span class="na">node-labels</span><span class="pi">:</span> <span class="s">metal3.io/uuid={{ ds.meta_data.uuid }}</span>
            <span class="na">provider-id</span><span class="pi">:</span> <span class="s">metal3://{{ ds.meta_data.uuid }}</span>
            <span class="na">runtime-request-timeout</span><span class="pi">:</span> <span class="s">5m</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">{{</span><span class="nv"> </span><span class="s">ds.meta_data.name</span><span class="nv"> </span><span class="s">}}"</span>
      <span class="na">preKubeadmCommands</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">netplan apply</span>
        <span class="pi">-</span> <span class="s">systemctl enable --now crio kubelet</span>
      <span class="na">users</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">metal3</span>
          <span class="c1"># sshAuthorizedKeys:</span>
          <span class="c1"># - add your public key here for debugging</span>
          <span class="na">sudo</span><span class="pi">:</span> <span class="s">ALL=(ALL) NOPASSWD:ALL</span>

</code></pre></div>    </div>

  </div>
</details>

<p>The result of all this is a Cluster with two Machines, one from the
Metal³ provider and one from the BYOH provider.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>k <span class="nt">-n</span> metal3 get machine
<span class="go">NAME                                CLUSTER         NODENAME                PROVIDERID                                      PHASE     AGE     VERSION
mixed-cluster-control-plane-48qmm   mixed-cluster   control-plane1          byoh://control-plane1/jf5uye                    Running   7m41s   v1.23.5
test1-8767dbccd-24cl5               mixed-cluster   test1-8767dbccd-24cl5   metal3://0642d832-3a7c-4ce9-833e-a629a60a455c   Running   7m18s   v1.23.5
</span></code></pre></div></div>

<p>Let’s also check that the workload cluster is functioning as expected.
Get the kubeconfig and add Calico as CNI.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>clusterctl get kubeconfig <span class="nt">-n</span> metal3 mixed-cluster <span class="o">&gt;</span> kubeconfig.yaml
<span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span>kubeconfig.yaml
kubectl apply <span class="nt">-f</span> https://docs.projectcalico.org/v3.20/manifests/calico.yaml
</code></pre></div></div>

<p>Now check the nodes.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>kubectl get nodes
<span class="go">NAME                    STATUS   ROLES                  AGE   VERSION
control-plane1          Ready    control-plane,master   88m   v1.23.5
</span><span class="gp">test1-8767dbccd-24cl5   Ready    &lt;none&gt;</span><span class="w">                 </span>82m   v1.23.5
</code></pre></div></div>

<p>Going back to the management cluster, we can inspect the state of the
cluster API resources.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">$</span><span class="w"> </span>clusterctl <span class="nt">-n</span> metal3 describe cluster mixed-cluster
<span class="go">NAME                                                                        READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/mixed-cluster                                                       True                     13m
├─ClusterInfrastructure - ByoCluster/mixed-cluster
├─ControlPlane - KubeadmControlPlane/mixed-cluster-control-plane            True                     13m
│ └─Machine/mixed-cluster-control-plane-hp2fp                               True                     13m
│   └─MachineInfrastructure - ByoMachine/mixed-cluster-control-plane-vxft5
└─Workers
  └─MachineDeployment/test1                                                 True                     3m57s
    └─Machine/test1-7f77dfb7c8-j7x4q                                        True                     9m32s
</span></code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>As we have seen in this post, it is possible to combine at least some
infrastructure providers when creating a single cluster. This can be
useful for example if a provider has a high cost or limited resources.
Furthermore, the use case is not addressed by MachineDeployments since
they would all be from the same provider (even though they can have
different properties).</p>

<p>There is some room for development and improvement though. The most
obvious thing is perhaps that Clusters only have one
<code class="language-plaintext highlighter-rouge">infrastructureRef</code>. This means that the cluster API controllers are not
aware of the “secondary” infrastructure provider(s).</p>

<p>Another thing that may be less obvious is the reliance on Nodes and
Machines in the Kubeadm control plane provider. It is not an issue in
the example we have seen here since both Metal³ and BYOH creates Nodes.
However, there are some projects where Nodes are unnecessary. See for
example <a href="https://github.com/clastix/kamaji">Kamaji</a>, which aims to
integrate with the cluster API. The idea here is to run the control
plane components in the management cluster as Pods. Naturally, there
would not be any control plane Nodes or Machines in this case. (A second
provider would be used to add workers.) But the Kubeadm control plane
provider expects there to be both Machines and Nodes for the control
plane, so a new provider is likely needed to make this work as desired.</p>

<p>This issue can already be seen in the
<a href="https://github.com/loft-sh/cluster-api-provider-vcluster">vcluster</a>
provider, where the Cluster stays in <code class="language-plaintext highlighter-rouge">Provisioning</code> state because it is
“Waiting for the first control plane machine to have its
<code class="language-plaintext highlighter-rouge">status.nodeRef</code> set”. The idea with vcluster is to reuse the Nodes of
the management cluster but provide a separate control plane. This gives
users better isolation than just namespaces without the need for another
“real” cluster. It is for example possible to have different custom
resource definitions in each vcluster. But since vcluster runs all the
pods (including the control plane) in the management cluster, there will
never be a control plane Machine or <code class="language-plaintext highlighter-rouge">nodeRef</code>.</p>

<p>There is already one implementation of a control plane provider without
Nodes, i.e. the EKS provider. Perhaps this is the way forward. One
implementation for each specific case. It would be nice if it was
possible to do it in a more generic way though, similar to how the
Kubeadm control plane provider is used by almost all infrastructure
providers.</p>

<p>To summarize, there is already some support for mixed clusters with
multiple providers. However, there are some issues that make it
unnecessarily awkward. Two things that could be improved in the cluster
API would be the following:</p>

<ol>
  <li>Make the <code class="language-plaintext highlighter-rouge">cluster.infrastructureRef</code> into a list to allow multiple
infrastructure providers to be registered.</li>
  <li>Drop the assumption that there will always be control plane Machines
and Nodes (e.g. by implementing a new control plane provider).</li>
</ol>]]></content><author><name>Lennart Jern</name></author><category term="metal3" /><category term="cluster API" /><category term="provider" /><category term="hybrid" /><category term="edge" /><summary type="html"><![CDATA[Running on bare metal has both benefits and drawbacks. You can get the best performance possible out of the hardware, but it can also be quite expensive and maybe not necessary for all workloads. Perhaps a hybrid cluster could give you the best of both? Raw power for the workload that needs it, and cheap virtualized commodity for the rest. This blog post will show how to set up a cluster like this using the Cluster API backed by the Metal3 and BYOH providers.]]></summary></entry><entry><title type="html">Metal3 Introduces Pivoting</title><link href="https://metal3.io/blog/2021/05/05/Pivoting.html" rel="alternate" type="text/html" title="Metal3 Introduces Pivoting" /><published>2021-05-05T00:00:00-05:00</published><updated>2021-05-05T00:00:00-05:00</updated><id>https://metal3.io/blog/2021/05/05/Pivoting</id><content type="html" xml:base="https://metal3.io/blog/2021/05/05/Pivoting.html"><![CDATA[<p>Metal3 project has introduced pivoting in its CI workflow. The motivation for
pivoting is to move all the objects from the ephemeral/management
cluster to a target cluster. This blog post will briefly introduce the concept
of pivoting and the impact it has on the overall CI workflow. For the rest of
this blog, we refer ephemeral/management cluster as an ephemeral cluster.</p>

<h2 id="what-is-pivoting">What is Pivoting?</h2>

<p>In the context of Metal3 Provider, Pivoting is the process of moving
Cluster-API and Metal3 objects from the ephemeral k8s cluster to a target
cluster. In Metal3, this process is performed using the
<a href="https://cluster-api.sigs.k8s.io/clusterctl/overview.html">clusterctl</a> tool
provided by Cluster-API. clusterctl recognizes pivoting as a move. During the
pivot process, clusterctl pauses any reconciliation of Cluster-API objects and
this gets propagated to Cluster-api-provider-metal3 (CAPM3) objects as well.
Once all the objects are paused, the objects are created on the other side on
the target cluster and deleted from the ephemeral cluster.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Prior to the actual pivot process, the target cluster should already have the
provider components, ironic containers and CNI installed and running. To perform
pivot outside metal3-dev-env, specifically, the following points need to be
addressed:</p>

<ul>
  <li>clusterctl is used to initialize both the ephemeral and target cluster.</li>
  <li>BMH objects have correct status annotation.</li>
  <li>Maintain connectivity towards the provisioning network.</li>
  <li>Baremetal Operator(BMO) is deployed as part of CAPM3.</li>
  <li>Objects should have a proper owner reference chain.</li>
</ul>

<p>For a detailed explanation of the above-mentioned prerequisites please read the
<a href="https://github.com/metal3-io/metal3-docs/blob/master/docs/move.md">pivoting documentation</a>.</p>

<h2 id="pivoting-workflow-in-ci">Pivoting workflow in CI</h2>

<p>The Metal3 CI currently includes pivoting as part of the deployment
process both for Ubuntu and CentOS-based jobs. This essentially means all
the PRs that go in, are tested through the pivoting workflow. Here is the
CI deployment workflow:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">make</code> the <a href="https://github.com/metal3-io/metal3-dev-env.git">metal3-dev-env</a>.
It gives us the ephemeral cluster with all the necessary controllers running
within it. The corresponding metal3-dev-env command is <code class="language-plaintext highlighter-rouge">make</code></li>
  <li><code class="language-plaintext highlighter-rouge">provision</code> target cluster. For normal integration tests, this step deploys
a control-plane node and a worker in the target cluster. For, <code class="language-plaintext highlighter-rouge">feature-test</code>
and <code class="language-plaintext highlighter-rouge">feature-test-upgrade</code> the provision step deploys three control-planes and
a worker. The corresponding metal3-dev-env commands are (normal integration
test workflow):</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/provision/cluster.sh
./scripts/provision/controlplane.sh
./scripts/provision/worker.sh
</code></pre></div></div>

<ul>
  <li>Initialize the provider components on the target cluster. This installs all
the controllers and associated components related to cluster-api ,
cluster-api-provider-metal3, baremetal-operator and ironic. Since it is
necessary to have only one set of ironic deployment/containers in the picture,
this step also deletes the ironic deployment/containers from
ephemeral cluster.</li>
  <li><code class="language-plaintext highlighter-rouge">Move</code> all the objects from ephemeral to the target cluster.</li>
  <li>Check the status of the objects to verify whether the objects are being
reconciled correctly by the controllers in the target cluster. This step
verifies and finalizes the pivoting process. The corresponding metal3-dev-env
the command that performs this and the previous two steps is :</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/feature_tests/pivoting/pivot.sh
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Move</code> the objects back to the ephemeral cluster. This step also
removes the ironic deployment from the target cluster and reinstates the
ironic deployment/containers in the ephemeral cluster. Since we do
not delete the provider components in the ephemeral cluster,
installing them again is not necessary. The corresponding metal3-dev-env command
that performs this step is :</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/feature_tests/pivoting/repivot.sh
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">De-provision</code> the BMHs and delete the target cluster. The corresponding
metal3-dev-env commands to de-provision worker, controlplane and the cluster
is as follows:</li>
</ul>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="syntax"><code>./scripts/deprovision/worker.sh
./scripts/deprovision/controlplane.sh
./scripts/deprovision/cluster.sh
</code></pre></div></div>

<p>Note that, if we de-provision cluster, that would de-provision worker and
controlplane automatically.</p>

<h2 id="pivoting-in-metal3">Pivoting in Metal3</h2>

<p>The pivoting process described above is realized in <code class="language-plaintext highlighter-rouge">ansible</code> scripts
<a href="https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/v1aX_integration_test/tasks/move.yml">move.yml</a>
and
<a href="https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/v1aX_integration_test/tasks/move_back.yml">move_back.yml</a>.
Under the hood, pivoting uses the <code class="language-plaintext highlighter-rouge">move</code> command from
<a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/move.html">clusterctl</a>
provided by Cluster-API.</p>

<p>As stated earlier, all the PRs that go into any Metal3 repository where the
integration tests are run, the code change introduced in the PR is verified with
pivoting also in the integration tests now. Moreover, the upgrade workflow in
Metal3 performs all the upgrade operations in Metal3 after pivoting to the
target cluster.</p>]]></content><author><name>Kashif Nizam Khan</name></author><category term="metal3" /><category term="baremetal" /><category term="Pivoting" /><category term="Move" /><summary type="html"><![CDATA[Metal3 project has introduced pivoting in its CI workflow. The motivation for pivoting is to move all the objects from the ephemeral/management cluster to a target cluster. This blog post will briefly introduce the concept of pivoting and the impact it has on the overall CI workflow. For the rest of this blog, we refer ephemeral/management cluster as an ephemeral cluster.]]></summary></entry><entry><title type="html">Introducing the Metal3 IP Address Manager</title><link href="https://metal3.io/blog/2020/07/06/IP_address_manager.html" rel="alternate" type="text/html" title="Introducing the Metal3 IP Address Manager" /><published>2020-07-06T00:00:00-05:00</published><updated>2020-07-06T00:00:00-05:00</updated><id>https://metal3.io/blog/2020/07/06/IP_address_manager</id><content type="html" xml:base="https://metal3.io/blog/2020/07/06/IP_address_manager.html"><![CDATA[<p>As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4
release, the Metal3 crew introduced a new project: its own IP Address Manager.
This blog post will go through the motivations behind such a project, the
features that it brings, its use in Metal3 and future work.</p>

<h2 id="what-is-the-ip-address-manager">What is the IP Address Manager?</h2>

<p>The IP Address Manager (IPAM) is a controller that provides IP addresses and
manages the allocations of IP subnets. It is not a DHCP server in that it only
reconciles Kubernetes objects and does not answer any DHCP queries. It
allocates IP addresses on request but does not handle any use of those
addresses.</p>

<p>This sounds like the description of any IPAM system, no? Well, the twist
is that this manager is based on Kubernetes to specifically handle some
constraints from Metal3. We will go through the different issues that this
project tackles.</p>

<p>When deploying nodes in a bare metal environment, there are a lot of possible
variations. This project specifically aims to solve cases where static
IP address configurations are needed. It is designed to specifically address
this in the <a href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI) context</a>.</p>

<p>CAPI addresses the deployment of Kubernetes clusters and nodes, using
the Kubernetes API. As such, it uses objects such as Machine Deployments
(similar to deployments for pods) that takes care of creating the requested
number of machines, based on templates. The replicas can be increased by the
user, triggering the creation of new machines based on the provided templates.
This mechanism does not allow for flexibility to be able to provide static
addresses for each machine. The manager adds this flexibility by providing
the address right before provisioning the node.</p>

<p>In addition, all the resources from the source cluster must support the CAPI
pivoting, i.e. being copied and recreated in the target cluster. This means
that all objects must contain all needed information in their spec field to
recreate the status in the target cluster without losing information. All
objects must, through a tree of owner references, be attached to the cluster
object, for the pivoting to proceed properly.</p>

<p>In a nutshell, the manager provides an IP Address allocation service, based
on Kubernetes API and fulfilling the needs of Metal3, specifically the
requirements of CAPI.</p>

<h2 id="how-does-it-work">How does it work?</h2>

<p>The manager follows the same logic as the volume allocation in Kubernetes,
with a claim and an object created for that claim. There are three types of
objects defined, the <code class="language-plaintext highlighter-rouge">IPPool</code>, the <code class="language-plaintext highlighter-rouge">IPClaim</code> and the <code class="language-plaintext highlighter-rouge">IPAddress</code> objects.</p>

<p>The <code class="language-plaintext highlighter-rouge">IPPool</code> objects contain the definition of the IP subnets from which the
Addresses are allocated. It supports both IPv4 and IPv6. The subnets can either
be defined as such or given as start and end IP addresses with a prefix.
It also supports pre-allocating IP addresses.</p>

<p>The following is an example <code class="language-plaintext highlighter-rouge">IPPool</code> definition :</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pool1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">cluster1</span>
  <span class="na">pools</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">start</span><span class="pi">:</span> <span class="s">192.168.0.10</span>
      <span class="na">end</span><span class="pi">:</span> <span class="s">192.168.0.30</span>
      <span class="na">prefix</span><span class="pi">:</span> <span class="m">25</span>
      <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.0.1</span>
    <span class="pi">-</span> <span class="na">subnet</span><span class="pi">:</span> <span class="s">192.168.1.1/26</span>
    <span class="pi">-</span> <span class="na">subnet</span><span class="pi">:</span> <span class="s">192.168.1.128/25</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.1.1</span>
  <span class="na">preAllocations</span><span class="pi">:</span>
    <span class="na">claim2</span><span class="pi">:</span> <span class="s">192.168.0.12</span>
</code></pre></div></div>

<p>An IPv6 <code class="language-plaintext highlighter-rouge">IPPool</code> would be defined similarly :</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pool1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">cluster1</span>
  <span class="na">pools</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">start</span><span class="pi">:</span> <span class="s">2001:0db8:85a3:0000:0000:8a2e::10</span>
      <span class="na">end</span><span class="pi">:</span> <span class="s">2001:0db8:85a3:0000:0000:8a2e:ffff:fff0</span>
      <span class="na">prefix</span><span class="pi">:</span> <span class="m">96</span>
      <span class="na">gateway</span><span class="pi">:</span> <span class="s">12001:0db8:85a3:0000:0000:8a2e::1</span>
    <span class="pi">-</span> <span class="na">subnet</span><span class="pi">:</span> <span class="s">2001:0db8:85a3:0000:0000:8a2d::/96</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">96</span>
  <span class="na">gateway</span><span class="pi">:</span> <span class="s">2001:0db8:85a3:0000:0000:8a2d::1</span>
</code></pre></div></div>

<p>Whenever something requires an IP address from the <code class="language-plaintext highlighter-rouge">IPPool</code>, it will create an
<code class="language-plaintext highlighter-rouge">IPClaim</code>. The <code class="language-plaintext highlighter-rouge">IPClaim</code> contains a pointer to the <code class="language-plaintext highlighter-rouge">IPPool</code> and an owner reference
to the object that created it.</p>

<p>The following is an example of an <code class="language-plaintext highlighter-rouge">IPClaim</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">claim1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">pool</span><span class="pi">:</span>
    <span class="na">Name</span><span class="pi">:</span> <span class="s">pool1</span>
<span class="na">status</span><span class="pi">:</span>
  <span class="na">address</span><span class="pi">:</span>
    <span class="na">Name</span><span class="pi">:</span> <span class="s">pool1-192-168-0-13</span>
</code></pre></div></div>

<p>The controller will then reconcile this object and allocate an IP address. It
will create an <code class="language-plaintext highlighter-rouge">IPAddress</code> object representing the allocated address. It will
then update the <code class="language-plaintext highlighter-rouge">IPPool</code> status to list the IP Address and the <code class="language-plaintext highlighter-rouge">IPClaim</code> status
to point to the <code class="language-plaintext highlighter-rouge">IPAddress</code>.</p>

<p>The following is an example of an <code class="language-plaintext highlighter-rouge">IPAddress</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPAddress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pool1-192-168-0-13</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">pool</span><span class="pi">:</span>
    <span class="na">Name</span><span class="pi">:</span> <span class="s">pool1</span>
  <span class="na">claim</span><span class="pi">:</span>
    <span class="na">Name</span><span class="pi">:</span> <span class="s">claim1</span>
  <span class="na">address</span><span class="pi">:</span> <span class="s">192.168.0.13</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.0.1</span>
</code></pre></div></div>

<p>After this allocation, the <code class="language-plaintext highlighter-rouge">IPPool</code> will be looking like this:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">ipam.metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">IPPool</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pool1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">cluster1</span>
  <span class="na">pools</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">start</span><span class="pi">:</span> <span class="s">192.168.0.10</span>
      <span class="na">end</span><span class="pi">:</span> <span class="s">192.168.0.30</span>
      <span class="na">prefix</span><span class="pi">:</span> <span class="m">25</span>
      <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.0.1</span>
    <span class="pi">-</span> <span class="na">subnet</span><span class="pi">:</span> <span class="s">192.168.1.1/26</span>
    <span class="pi">-</span> <span class="na">subnet</span><span class="pi">:</span> <span class="s">192.168.1.128/25</span>
  <span class="na">prefix</span><span class="pi">:</span> <span class="m">24</span>
  <span class="na">gateway</span><span class="pi">:</span> <span class="s">192.168.1.1</span>
  <span class="na">preAllocations</span><span class="pi">:</span>
    <span class="na">claim2</span><span class="pi">:</span> <span class="s">192.168.0.12</span>
<span class="na">status</span><span class="pi">:</span>
  <span class="na">indexes</span><span class="pi">:</span>
    <span class="na">claim1</span><span class="pi">:</span> <span class="s">192.168.0.13</span>
    <span class="na">claim2</span><span class="pi">:</span> <span class="s">192.168.0.12</span>
</code></pre></div></div>

<h2 id="use-in-metal3">Use in Metal3</h2>

<p>The IP Address Manager is used in Metal3 together with the metadata and network
data templates feature. Each Metal3Machine (M3M) and Metal3MachineTemplate
(M3MT) is associated with a Metal3DataTemplate that contains metadata and /
or a network data template that will be rendered for each Metal3Machine. The
rendered data will then be provided to Ironic. Those templates reference
<code class="language-plaintext highlighter-rouge">IPPool</code> objects. For each Metal3Machine, an <code class="language-plaintext highlighter-rouge">IPClaim</code> is created for each
<code class="language-plaintext highlighter-rouge">IPPool</code>, and the templates are rendered with the allocated <code class="language-plaintext highlighter-rouge">IPAddress</code>.</p>

<p>This is how we achieve dynamic IP Address allocations in setups that
require static configuration, allowing us to use Machine Deployment and Kubeadm
Control Plane objects from CAPI in hardware labs where DHCP is not supported.</p>

<p>Since each <code class="language-plaintext highlighter-rouge">IPAddress</code> has an owner reference set to its <code class="language-plaintext highlighter-rouge">IPClaim</code> object, and
<code class="language-plaintext highlighter-rouge">IPClaim</code> objects have an owner reference set to the Metal3Data object created
from the Metal3DataTemplate, the owner reference chain links a Metal3Machine to
all the <code class="language-plaintext highlighter-rouge">IPClaim</code> and <code class="language-plaintext highlighter-rouge">IPAddress</code> objects were created for it, allowing for CAPI
pivoting.</p>

<h2 id="what-now">What now?</h2>

<p>The project is fulfilling its basic requirements, but we are looking into
extending it and covering more use cases. For example, we are looking at
adding integration with Infoblox and other external IPAM services. Do not
hesitate to open an issue if you have some ideas for new features!</p>

<p>The project can be found
<a href="https://github.com/metal3-io/ip-address-manager">here</a>.</p>]]></content><author><name>Maël Kimmerlin</name></author><category term="metal3" /><category term="baremetal" /><category term="IPAM" /><category term="ip address manager" /><summary type="html"><![CDATA[As a part of developing the Cluster API Provider Metal3 (CAPM3) v1alpha4 release, the Metal3 crew introduced a new project: its own IP Address Manager. This blog post will go through the motivations behind such a project, the features that it brings, its use in Metal3 and future work.]]></summary></entry><entry><title type="html">Raw image streaming available in Metal3</title><link href="https://metal3.io/blog/2020/07/05/raw-image-streaming.html" rel="alternate" type="text/html" title="Raw image streaming available in Metal3" /><published>2020-07-05T00:00:00-05:00</published><updated>2020-07-05T00:00:00-05:00</updated><id>https://metal3.io/blog/2020/07/05/raw-image-streaming</id><content type="html" xml:base="https://metal3.io/blog/2020/07/05/raw-image-streaming.html"><![CDATA[<p>Metal3 supports multiple types of images for deployment, the most
popular being QCOW2. We have recently added support for a feature of Ironic
that improves deployments on constrained environments, raw image streaming.
We’ll first dive into how Ironic deploys the images on the target hosts, and
how raw image streaming improves this process. Afterwards, we will point out
the changes to take this into use in Metal3.</p>

<h2 id="image-deployments-with-ironic">Image deployments with Ironic</h2>

<p>In Metal3, the image deployment is performed by the Ironic Python Agent (IPA)
image running on the target host. In order to deploy an image, Ironic will
first boot the target node with an IPA image over iPXE. IPA will run in memory.</p>

<p>Once IPA runs on the target node, Ironic will instruct it to download the
target image. In Metal3, we use HTTP(S) for the download of the image. IPA will
download the image and, depending on the format of the image, prepare it to
write on the disk. This means that the image is downloaded in memory and
decompressed, two steps that can be both time and memory consuming.</p>

<p>In order to improve this process, Ironic implemented a feature called raw image
streaming.</p>

<h2 id="what-is-raw-image-streaming">What is raw image streaming?</h2>

<p>The target image format when writing to disk is raw. That’s why the images in
formats like QCOW2 must be processed before being written to disk. However, if
the image that is downloaded is already in raw format, then no processing is
needed.</p>

<p>Ironic leverages this, and instead of first downloading the image and then
processing it before writing it to disk, it will directly write the
downloaded image to the disk. This feature is known as image streaming.
Image streaming can only be performed with images in raw format.</p>

<p>Since the downloaded image when streamed is directly written to disk, the
memory size requirements change. For any other format than raw, the target
host needs to have sufficient memory to both run IPA (4GB) and
download the image in memory. However, with raw images, the only constraint
on memory is to run IPA (so 4GB). For example, in order to deploy an Ubuntu
image (around 700MB, QCOW2), the requirement is 8GB when in QCOW2 format, while
it is only 4GB (as for any other image) when streamed as raw. This allows
the deployment of images that are bigger than the available memory on
constrained nodes.</p>

<p>However, this shifts the load on the network, since the raw images are usually
much bigger than other formats. Using this feature in network constrained
environment is not recommended.</p>

<h2 id="raw-image-streaming-in-metal3">Raw image streaming in Metal3</h2>

<p>In order to use raw image streaming in Metal3, a couple of steps are needed.
The first one is to convert the image to raw and make it available in an
HTTP server. This can be achieved by running :</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>qemu-img convert <span class="nt">-O</span> raw <span class="s2">"</span><span class="k">${</span><span class="nv">IMAGE_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">IMAGE_RAW_NAME</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<p>Once converted the image format needs to be provided to Ironic through the
BareMetalHost (BMH) image spec field. If not provided, Ironic will assume that
the format is unspecified and download it in memory first.</p>

<p>The following is an example of the BMH image spec field in Metal3 Dev Env.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">metal3.io/v1alpha1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">BareMetalHost</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">image</span><span class="pi">:</span>
    <span class="na">format</span><span class="pi">:</span> <span class="s">raw</span>
    <span class="na">url</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img</span>
    <span class="na">checksum</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img.md5sum</span>
    <span class="na">checksumType</span><span class="pi">:</span> <span class="s">md5</span>
</code></pre></div></div>

<p>If deploying with Cluster API provider Metal3 (CAPM3), CAPM3 takes care of
setting the image field of BMH properly, based on the image field values in
the Metal3Machine (M3M), which might be based on a Metal3MachineTemplate (M3MT).
So in order to use raw image streaming, the format of the image must be
provided in the image spec field of the Metal3Machine or Metal3MachineTemplate.</p>

<p>The following is an example of the M3M image spec field in metal3-dev-env :</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3Machine</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">image</span><span class="pi">:</span>
    <span class="na">format</span><span class="pi">:</span> <span class="s">raw</span>
    <span class="na">url</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img</span>
    <span class="na">checksum</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img.md5sum</span>
    <span class="na">checksumType</span><span class="pi">:</span> <span class="s">md5</span>
</code></pre></div></div>

<p>The following is for a M3MT in metal3-dev-env :</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">image</span><span class="pi">:</span>
        <span class="na">format</span><span class="pi">:</span> <span class="s">raw</span>
        <span class="na">url</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img</span>
        <span class="na">checksum</span><span class="pi">:</span> <span class="s">http://172.22.0.1/images/bionic-server-cloudimg-amd64-raw.img.md5sum</span>
        <span class="na">checksumType</span><span class="pi">:</span> <span class="s">md5</span>
</code></pre></div></div>

<p>This will enable raw image streaming. By default, metal3-dev-env uses the raw image
streaming, in order to minimize the resource requirements of the environment.</p>

<h2 id="in-a-nutshell">In a nutshell</h2>

<p>With the addition of raw image streaming, Metal3 now supports a wider range of
hardware, specifically, the memory-constrained nodes and speeds up deployments.
Metal3 still supports all the other formats it supported until now. This new
feature changes the way raw images are deployed for better efficiency.</p>]]></content><author><name>Maël Kimmerlin</name></author><category term="metal3" /><category term="baremetal" /><category term="raw image" /><category term="image streaming" /><summary type="html"><![CDATA[Metal3 supports multiple types of images for deployment, the most popular being QCOW2. We have recently added support for a feature of Ironic that improves deployments on constrained environments, raw image streaming. We’ll first dive into how Ironic deploys the images on the target hosts, and how raw image streaming improves this process. Afterwards, we will point out the changes to take this into use in Metal3.]]></summary></entry><entry><title type="html">Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster</title><link href="https://metal3.io/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment.html" rel="alternate" type="text/html" title="Metal³ development environment walkthrough part 2: Deploying a new bare metal cluster" /><published>2020-06-18T00:00:00-05:00</published><updated>2020-06-18T00:00:00-05:00</updated><id>https://metal3.io/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment</id><content type="html" xml:base="https://metal3.io/blog/2020/06/18/Metal3-dev-env-BareMetal-Cluster-Deployment.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>This blog post describes how to deploy a bare metal cluster, a virtual
one for simplicity, using
<a href="https://github.com/metal3-io/metal3-dev-env">Metal³/metal3-dev-env</a>. We
will briefly discuss the steps involved in setting up the cluster as
well as some of the customization available. If you want to know more
about the architecture of Metal³, this <a href="/blog/2020/02/27/talk-kubernetes-finland-metal3.html">blogpost</a> can be helpful.</p>

<p>This post builds upon the <a href="/blog/2020/02/18/metal3-dev-env-install-deep-dive.html">detailed metal3-dev-env walkthrough
blogpost</a>
which describes in detail the steps involved in the environment set-up
and management cluster configuration. Here we will use that environment
to deploy a new Kubernetes cluster using Metal³.</p>

<p>Before we get started, there are a couple of requirements we are
expecting to be fulfilled.</p>

<h2 id="requirements">Requirements</h2>

<ul>
  <li>Metal³ is already deployed and working, if not please follow the
instructions in the previously mentioned <a href="/blog/2020/02/18/metal3-dev-env-install-deep-dive.html">detailed metal3-dev-env
walkthrough blogpost</a>.</li>
  <li>The appropriate environment variables are setup via shell or in the
<code class="language-plaintext highlighter-rouge">config_${user}.sh</code> file, for example
    <ul>
      <li>CAPM3_VERSION</li>
      <li>NUM_NODES</li>
      <li>CLUSTER_NAME</li>
    </ul>
  </li>
</ul>

<h2 id="overview-of-config-and-resource-types">Overview of Config and Resource types</h2>

<p>In this section, we give a brief overview of the important config files
and resources used as part of the bare metal cluster deployment. The
following sub-sections show the config files and resources that are
created and give a brief description of some of them. This will help you
understand the technical details of the cluster deployment. You can also
choose to skip this section, visit the next section about <em>provisioning</em>
first and then revisit this.</p>

<h3 id="config-files-and-resources-types">Config Files and Resources Types</h3>

<p><img src="/assets/2020-06-18-Metal3-dev-env-BareMetal-Cluster-Deployment/manifest-directory.png" alt="&quot;The directory tree for the ansible role used for deployment&quot;" /></p>

<blockquote>
  <p>info “Information” Among these the config files are rendered under the
path
<code class="language-plaintext highlighter-rouge">https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/files</code>
as part of the provisioning process.</p>
</blockquote>

<p>A description of some of the files part of provisioning a cluster, in a
centos-based environment:</p>

<!-- markdownlint-disable MD013 -->

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Description</th>
      <th>Path</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>provisioning scripts</td>
      <td>Scripts to trigger provisioning of cluster, control plane or worker</td>
      <td><code class="language-plaintext highlighter-rouge">${metal3-dev-env}/scripts/provision/</code></td>
    </tr>
    <tr>
      <td>deprovisioning scripts</td>
      <td>Scripts to trigger deprovisioning of cluster, control plane or worker</td>
      <td><code class="language-plaintext highlighter-rouge">${metal3-dev-env}/scripts/deprovision/</code></td>
    </tr>
    <tr>
      <td><a href="https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/templates">templates directory</a></td>
      <td>Templates for cluster, control plane, worker definitions</td>
      <td><code class="language-plaintext highlighter-rouge">${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/templates</code></td>
    </tr>
    <tr>
      <td>clusterctl env file</td>
      <td>Cluster parameters and details</td>
      <td><code class="language-plaintext highlighter-rouge">${Manifests}/clusterctl_env_centos.rc</code></td>
    </tr>
    <tr>
      <td><a href="https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/tasks/generate_templates.yml">generate templates</a></td>
      <td>Renders cluster, control plane and worker definitions in the <code class="language-plaintext highlighter-rouge">Manifest</code> directory</td>
      <td><code class="language-plaintext highlighter-rouge">${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/tasks/generate_templates.yml</code></td>
    </tr>
    <tr>
      <td><a href="https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/vars/main.yml">main vars file</a></td>
      <td>Variable file that assigns all the defaults used during deployment</td>
      <td><code class="language-plaintext highlighter-rouge">${metal3-dev-env}/vm-setup/roles/v1aX_integration_test/vars/main.yml</code></td>
    </tr>
  </tbody>
</table>

<p>Here are some of the resources that are created as part of provisioning :</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Cluster</td>
      <td>a Cluster API resource for managing a cluster</td>
    </tr>
    <tr>
      <td>Metal3Cluster</td>
      <td>Corresponding Metal3 resource generated as part of bare metal cluster deployment, and managed by <code class="language-plaintext highlighter-rouge">Cluster</code></td>
    </tr>
    <tr>
      <td>KubeadmControlPlane</td>
      <td>Cluster API resource for managing the control plane, it also manages the <code class="language-plaintext highlighter-rouge">Machine</code> object, and has the <strong>KubeadmConfig</strong></td>
    </tr>
    <tr>
      <td>MachineDeployment</td>
      <td>Cluster API resource for managing workers via <code class="language-plaintext highlighter-rouge">MachineSet</code> object, it can be used to add/remove workers by scaling Up/Down</td>
    </tr>
    <tr>
      <td>MachineSet</td>
      <td>Cluster API resource for managing <code class="language-plaintext highlighter-rouge">Machine</code> objects for worker nodes</td>
    </tr>
    <tr>
      <td>Machine</td>
      <td>Cluster API resource for managing nodes - control plane or workers. In case of Controlplane, its directly managed by <code class="language-plaintext highlighter-rouge">KubeadmControlPlane</code>, whereas for Workers it’s managed by a <code class="language-plaintext highlighter-rouge">MachineSet</code></td>
    </tr>
    <tr>
      <td>Metal3Machine</td>
      <td>Corresponding Metal3 resource for managing bare metal nodes, it’s managed by a <code class="language-plaintext highlighter-rouge">Machine</code> resource</td>
    </tr>
    <tr>
      <td>Metal3MachineTemplate</td>
      <td>Metal3 resource which acts as a template when creating a control plane or a worker node</td>
    </tr>
    <tr>
      <td>KubeadmConfigTemplate</td>
      <td>A template of <code class="language-plaintext highlighter-rouge">KubeadmConfig</code>, for Workers, used to generate KubeadmConfig when a new worker node is provisioned</td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-enable MD013 -->

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>The corresponding <code class="language-plaintext highlighter-rouge">KubeadmConfig</code> is copied to the control
plane/worker at the time of provisioning.</p>


</div></div>
<h2 id="bare-metal-cluster-deployment">Bare Metal Cluster Deployment</h2>

<p>The deployment scripts primarily use ansible and the existing Kubernetes
management cluster (based on minikube ) for deploying the bare-metal
cluster. Make sure that some of the environment variables used for
Metal³ deployment are set, if you didn’t use <code class="language-plaintext highlighter-rouge">config_${user}.sh</code> for
setting the environment variables.</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CAPM3_VERSION</td>
      <td>Version of Metal3 API</td>
      <td>v1alpha3</td>
    </tr>
    <tr>
      <td>POD_CIDR</td>
      <td>Pod Network CIDR</td>
      <td>192.168.0.0/18</td>
    </tr>
    <tr>
      <td>CLUSTER_NAME</td>
      <td>Name of bare metal cluster</td>
      <td>test1</td>
    </tr>
  </tbody>
</table>

<p>===</p>

<h3 id="steps-involved">Steps Involved</h3>

<p>All the scripts for cluster provisioning or de-provisioning are located
at -
<a href="https://github.com/metal3-io/metal3-dev-env/tree/master/scripts"><code class="language-plaintext highlighter-rouge">${metal3-dev-env}/scripts/</code></a>.
The scripts call a common playbook which handles all the tasks that are
available.</p>

<p>The steps involved in the process are:</p>

<ul>
  <li>The script calls an ansible playbook with necessary parameters ( from
env variables and defaults )</li>
  <li>The playbook executes the role -,
<a href="https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test"><code class="language-plaintext highlighter-rouge">${metal3-dev-env}/vm-setup/roles/v1aX_integration_test</code></a>,
which runs the main
<a href="https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/tasks/main.yml">task_file</a>
for provisioning/deprovisioning the cluster, control plane or a worker</li>
  <li>There are
<a href="https://github.com/metal3-io/metal3-dev-env/tree/master/vm-setup/roles/v1aX_integration_test/templates">templates</a>
in the role, which are used to render configurations in the <code class="language-plaintext highlighter-rouge">Manifest</code>
directory. These configurations use kubeadm and are supplied to the
Kubernetes module of ansible to create the cluster.</li>
  <li>During provisioning, first the <code class="language-plaintext highlighter-rouge">clusterctl</code> env file is generated,
then the cluster, control plane and worker definition templates for
<code class="language-plaintext highlighter-rouge">clusterctl</code> are generated at
<code class="language-plaintext highlighter-rouge">${HOME}/.cluster-api/overrides/infrastructure-metal3/${CAPM3RELEASE}</code>.</li>
  <li>Using the templates generated in the previous step, the definitions
for resources related to cluster, control plane and worker are
rendered using <code class="language-plaintext highlighter-rouge">clusterctl</code>.</li>
  <li>Centos or Ubuntu image <a href="https://github.com/metal3-io/metal3-dev-env/blob/master/vm-setup/roles/v1aX_integration_test/tasks/download_image.yml">is
downloaded</a>
in the next step.</li>
  <li>Finally using the above definitions, which are passed to the <code class="language-plaintext highlighter-rouge">K8s</code>
module in ansible, the corresponding resource( cluster/control
plane/worker ) is provisioned.</li>
  <li>These same definitions are reused at the time of de-provisioning the
corresponding resource, again using the <code class="language-plaintext highlighter-rouge">K8s</code> module in ansible
    <blockquote>
      <p>note “Note” The manifest directory is created when provisioning is
triggered for the first time and is subsequently used to store the
config files that are rendered for deploying the bare metal cluster.</p>
    </blockquote>
  </li>
</ul>

<p><img src="/assets/2020-06-18-Metal3-dev-env-BareMetal-Cluster-Deployment/metal3-bmetal-arch-overview.png" alt="&quot;An Overview of various resources generated while provisioning and
their relationship amongst
themselves&quot;" /></p>

<h3 id="provision-cluster">Provision Cluster</h3>

<p>This script, located at the path -
<code class="language-plaintext highlighter-rouge">${metal3-dev-env}/scripts/provision/cluster.sh</code>, provisions the cluster
by creating a <code class="language-plaintext highlighter-rouge">Metal3Cluster</code> and a <code class="language-plaintext highlighter-rouge">Cluster</code> resource.</p>

<p>To see if you have a successful Cluster resource creation( the cluster
still doesn’t have a control plane or workers ), just do:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">kubectl get Metal3Cluster $</span><span class="o">{</span>CLUSTER_NAME<span class="o">}</span> <span class="nt">-n</span> metal3
</code></pre></div></div>

<blockquote>
  <p>This will return the cluster deployed, and you can check the cluster
details by describing the returned resource.</p>
</blockquote>

<p>Here is what a <code class="language-plaintext highlighter-rouge">Cluster</code> resource looks like:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">kubectl describe Cluster $</span><span class="o">{</span>CLUSTER_NAME<span class="o">}</span> <span class="nt">-n</span> metal3
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Cluster</span>
<span class="na">metadata</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">......</span><span class="pi">]</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterNetwork</span><span class="pi">:</span>
    <span class="na">pods</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">192.168.0.0/18</span>
    <span class="na">services</span><span class="pi">:</span>
      <span class="na">cidrBlocks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">10.96.0.0/12</span>
  <span class="na">controlPlaneEndpoint</span><span class="pi">:</span>
    <span class="na">host</span><span class="pi">:</span> <span class="s">192.168.111.249</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">6443</span>
  <span class="na">controlPlaneRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1alpha3</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
    <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
  <span class="na">infrastructureRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1alpha3</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3Cluster</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
    <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
<span class="na">status</span><span class="pi">:</span>
  <span class="na">infrastructureReady</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">phase</span><span class="pi">:</span> <span class="s">Provisioned</span>
</code></pre></div></div>

<h3 id="provision-controlplane">Provision Controlplane</h3>

<p>This script, located at the path -
<code class="language-plaintext highlighter-rouge">${metal3-dev-env}/scripts/provision/controlplane.sh</code>, provisions the
control plane member of the cluster using the rendered definition of the
control plane explained in the <strong>Steps Involved</strong> section. The
<code class="language-plaintext highlighter-rouge">KubeadmControlPlane</code> creates a <code class="language-plaintext highlighter-rouge">Machine</code> which picks up a BareMetalHost
satisfying its requirements as the control plane node, and it is then
provisioned by the Bare Metal Operator. A <code class="language-plaintext highlighter-rouge">Metal3MachineTemplate</code>
resource is also created as part of the provisioning process.</p>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>It takes some time for the provisioning of the control plane, you can
watch the process using some steps shared a bit later</p>


</div></div>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">kubectl get KubeadmControlPlane $</span><span class="o">{</span>CLUSTER_NAME<span class="o">}</span> <span class="nt">-n</span> metal3
<span class="gp">kubectl describe KubeadmControlPlane $</span><span class="o">{</span>CLUSTER_NAME<span class="o">}</span> <span class="nt">-n</span> metal3
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">controlplane.cluster.x-k8s.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmControlPlane</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="pi">[</span><span class="nv">....</span><span class="pi">]</span>
  <span class="na">ownerReferences</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1alpha3</span>
    <span class="na">blockOwnerDeletion</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">controller</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">Cluster</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
    <span class="na">uid</span><span class="pi">:</span> <span class="s">aec0f73b-a068-4992-840d-6330bf943d22</span>
  <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">44555"</span>
  <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/apis/controlplane.cluster.x-k8s.io/v1alpha3/namespaces/metal3/kubeadmcontrolplanes/bmetalcluster</span>
  <span class="na">uid</span><span class="pi">:</span> <span class="s">99487c75-30f1-4765-b895-0b83b0e5402b</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">infrastructureTemplate</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1alpha3</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bmetalcluster-controlplane</span>
    <span class="na">namespace</span><span class="pi">:</span> <span class="s">metal3</span>
  <span class="na">kubeadmConfigSpec</span><span class="pi">:</span>
    <span class="na">files</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">content</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">[....]</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">version</span><span class="pi">:</span> <span class="s">v1.18.0</span>
<span class="na">status</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/cluster-name=bmetalcluster,cluster.x-k8s.io/control-plane=</span>
  <span class="na">unavailableReplicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">updatedReplicas</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">kubectl get Metal3MachineTemplate $</span><span class="o">{</span>CLUSTER_NAME<span class="o">}</span><span class="nt">-controlplane</span> <span class="nt">-n</span> metal3
</code></pre></div></div>

<p>To track the progress of provisioning, you can try the following:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">kubectl get BareMetalHosts -n metal3 -w
</span></code></pre></div></div>

<blockquote>
  <p>The <code class="language-plaintext highlighter-rouge">BareMetalHosts</code> resource is created when <code class="language-plaintext highlighter-rouge">Metal³/metal3-dev-env</code>
was deployed. It is a kubernetes resource that represents a bare metal
Machine, with all its details and configuration, and is managed by the
<code class="language-plaintext highlighter-rouge">Bare Metal Operator</code>. You can also use the short representation
instead, i.e. <code class="language-plaintext highlighter-rouge">bmh</code> ( short for <code class="language-plaintext highlighter-rouge">BareMetalHosts</code>) in the command
above.
You should see all the nodes that were created at the time of metal3
deployment, along with their current status as the provisioning
progresses</p>
</blockquote>
<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>All the bare metal hosts listed above were created when Metal³ was
deployed in the <em>detailed metal3-dev-env walkthrough blogpost</em>.</p>


</div></div>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">kubectl get Machine -n metal3 -w
</span></code></pre></div></div>

<blockquote>
  <p>This shows the status of the Machine associated with the control plane
and we can watch the status of provisioning under PHASE</p>
</blockquote>

<p>Once the provisioning is finished, let’s get the host-ip:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">sudo virsh net-dhcp-leases baremetal
</span></code></pre></div></div>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p><code class="language-plaintext highlighter-rouge">baremetal</code> is one of the 2 networks that were created at the time of
Metal3 deployment, the other being “provisioning” which is used - as
you have guessed - for provisioning the bare metal cluster. More
details about networking setup in the metal3-dev-env environment are
described in the - <a href="/blog/2020/02/18/metal3-dev-env-install-deep-dive.html">detailed metal3-dev-env walkthrough
blogpost</a>.</p>


</div></div>
<p>You can log in to the control plane node if you want, and can check the
deployment status using two methods.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">ssh metal3@{control-plane-node-ip}
ssh metal3@192.168.111.249
</span></code></pre></div></div>

<h3 id="provision-workers">Provision Workers</h3>

<p>The script is located at
<code class="language-plaintext highlighter-rouge">${metal3-dev-env-path}/scripts/provision/worker.sh</code> and it provisions a
node to be added as a worker to the bare metal cluster. It selects one
of the remaining nodes and provisions it and adds it to the bare metal
cluster ( which only has a control plane node at this point ). The
resources created for workers are - <code class="language-plaintext highlighter-rouge">MachineDeployment</code> which can be
scaled up to add more workers to the cluster and <code class="language-plaintext highlighter-rouge">MachineSet</code> which then
creates a <code class="language-plaintext highlighter-rouge">Machine</code> managing the node.</p>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Information</p><p>Similar to control plane provisioning, worker provisioning also takes
some time, and you can watch the process using steps shared a bit
later. This will also apply when you scale Up/Down workers at a later
point in time.</p>


</div></div>
<p>This is what a <code class="language-plaintext highlighter-rouge">MachineDeployment</code> looks like</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">kubectl describe MachineDeployment $</span><span class="o">{</span>CLUSTER_NAME<span class="o">}</span> <span class="nt">-n</span> metal3
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1alpha3</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">MachineDeployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="pi">[</span><span class="nv">....</span><span class="pi">]</span>
  <span class="na">ownerReferences</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/v1alpha3</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">Cluster</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
    <span class="na">uid</span><span class="pi">:</span> <span class="s">aec0f73b-a068-4992-840d-6330bf943d22</span>
  <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">66257"</span>
  <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/apis/cluster.x-k8s.io/v1alpha3/namespaces/metal3/machinedeployments/bmetalcluster</span>
  <span class="na">uid</span><span class="pi">:</span> <span class="s">f598da43-0afe-44e4-b793-cd5244c13f4e</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterName</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
  <span class="na">minReadySeconds</span><span class="pi">:</span> <span class="m">0</span>
  <span class="na">progressDeadlineSeconds</span><span class="pi">:</span> <span class="m">600</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">revisionHistoryLimit</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
      <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
  <span class="na">strategy</span><span class="pi">:</span>
    <span class="na">rollingUpdate</span><span class="pi">:</span>
      <span class="na">maxSurge</span><span class="pi">:</span> <span class="m">1</span>
      <span class="na">maxUnavailable</span><span class="pi">:</span> <span class="m">0</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">RollingUpdate</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">cluster.x-k8s.io/cluster-name</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
        <span class="na">nodepool</span><span class="pi">:</span> <span class="s">nodepool-0</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">bootstrap</span><span class="pi">:</span>
        <span class="na">configRef</span><span class="pi">:</span>
          <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">bootstrap.cluster.x-k8s.io/v1alpha3</span>
          <span class="na">kind</span><span class="pi">:</span> <span class="s">KubeadmConfigTemplate</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">bmetalcluster-workers</span>
      <span class="na">clusterName</span><span class="pi">:</span> <span class="s">bmetalcluster</span>
      <span class="na">infrastructureRef</span><span class="pi">:</span>
        <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">infrastructure.cluster.x-k8s.io/v1alpha3</span>
        <span class="na">kind</span><span class="pi">:</span> <span class="s">Metal3MachineTemplate</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">bmetalcluster-workers</span>
      <span class="na">version</span><span class="pi">:</span> <span class="s">v1.18.0</span>
<span class="na">status</span><span class="pi">:</span>
  <span class="na">observedGeneration</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">phase</span><span class="pi">:</span> <span class="s">ScalingUp</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span> <span class="s">cluster.x-k8s.io/cluster-name=bmetalcluster,nodepool=nodepool-0</span>
  <span class="na">unavailableReplicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">updatedReplicas</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<p>To check the status we can follow steps similar to Controlplane case:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">kubectl get bmh -n metal3 -w
</span></code></pre></div></div>

<blockquote>
  <p>We can see the live status of the node being provisioned. As mentioned
before <code class="language-plaintext highlighter-rouge">bmh</code> is the short representation of <code class="language-plaintext highlighter-rouge">BareMetalHosts</code>.</p>
</blockquote>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">kubectl get Machine -n metal3 -w
</span></code></pre></div></div>

<blockquote>
  <p>This shows the status of Machines associated with workers, apart from
the one for Controlplane, and we can watch the status of provisioning
under PHASE</p>
</blockquote>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">sudo virsh net-dhcp-leases baremetal
</span></code></pre></div></div>

<blockquote>
  <p>To get the node’s IP</p>
</blockquote>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">ssh metal3@{control-plane-node-ip}
kubectl get nodes
</span></code></pre></div></div>

<blockquote>
  <p>To check if it’s added to the cluster</p>
</blockquote>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="go">ssh metal3@{node-ip}
</span></code></pre></div></div>

<blockquote>
  <p>If you want to log in to the node</p>
</blockquote>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">kubectl scale --replicas=3 MachineDeployment $</span><span class="o">{</span>CLUSTER_NAME<span class="o">}</span> <span class="nt">-n</span> metal3
</code></pre></div></div>

<blockquote>
  <p>We can add or remove workers to the cluster, and we can scale up the
MachineDeployment up or down, in this example we are adding 2 more
worker nodes, making the total nodes = 3</p>
</blockquote>

<h3 id="deprovisioning">Deprovisioning</h3>

<p>All of the previous components have corresponding de-provisioning
scripts which use config files, in the previously mentioned manifest
directory, and use them to clean up the worker, control plane and
cluster.</p>

<p>This step will use the already generated cluster/control plane/worker
definition file, and supply it to <strong>Kubernetes</strong> ansible module to
remove/de-provision the resource. You can find it, under the <code class="language-plaintext highlighter-rouge">Manifest</code>
directory, in the Snapshot shared at the beginning of this blogpost
where we show the file structure.</p>

<p>For example, if you wish to de-provision the cluster, you would do:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="gp">sh $</span><span class="o">{</span>metal3-dev-env-path<span class="o">}</span>/scripts/deprovision/worker.sh
<span class="gp">sh $</span><span class="o">{</span>metal3-dev-env-path<span class="o">}</span>/scripts/deprovision/controlplane.sh
<span class="gp">sh $</span><span class="o">{</span>metal3-dev-env-path<span class="o">}</span>/scripts/deprovision/cluster.sh
</code></pre></div></div>

<div class="premonition note"><div class="fa fa-check-square"></div><div class="content"><p class="header">Note</p><p>The reason for running the <code class="language-plaintext highlighter-rouge">deprovision/worker.sh</code> and
<code class="language-plaintext highlighter-rouge">deprovision/controlplane.sh</code> scripts is that not all objects are
cleared when we just run the <code class="language-plaintext highlighter-rouge">deprovision/cluster.sh</code> script.
Following this, if you want to de-provision the control plane it is
recommended to de-provision the cluster itself since we can’t
provision a new control plane with the same cluster. For worker
de-provisioning, we only need to run the worker script.</p>


</div></div>
<p>The following video demonstrates all the steps to provision and
de-provision a Kubernetes cluster explained above.</p>

<!-- markdownlint-disable MD033 MD013 -->

<iframe width="1110" height="625" style="height: 625px" src="https://www.youtube.com/embed/FzDQs_9XvtU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<!-- markdownlint-enable MD033 MD013 -->

<h2 id="summary">Summary</h2>

<p>In this blogpost we saw how to deploy a bare metal cluster once we have
a Metal³(metal3-dev-env repo) deployed and by that point we will already
have the nodes ready to be used for a bare metal cluster deployment.</p>

<p>In the first section, we show the various configuration files,
templates, resource types and their meanings. Then we see the common
steps involved in the provisioning process. After that, we see a general
overview of how all resources are related and at what point are they
created - provision cluster/control plane/worker.</p>

<p>In each of the provisioning sections, we see the steps to monitor the
provisioning and how to confirm if it’s successful or not, with brief
explanations wherever required. Finally, we see the de-provisioning
section which uses the resource definitions generated at the time of
provisioning to de-provision cluster, control plane or worker.</p>

<p>Here are a few resources which you might find useful if you want to
explore further, some of them have already been shared earlier.</p>

<ul>
  <li><a href="https://metal3.io/">Metal3-Documentation</a>
    <ul>
      <li><a href="https://book.metal3.io/developer_environment/tryit.html">Metal3-Try-it</a></li>
    </ul>
  </li>
  <li><a href="https://github.com/metal3-io/metal3-dev-env">Metal³/metal3-dev-env</a></li>
  <li><a href="/blog/2020/02/18/metal3-dev-env-install-deep-dive.html">Detailed metal3-dev-env walkthrough blogpost</a></li>
  <li><a href="/blog/2020/02/27/talk-kubernetes-finland-metal3.html">Kubernetes Metal3 Talk</a></li>
  <li><a href="https://github.com/metal3-io/metal3-docs">Metal3-Docs-github</a></li>
</ul>]]></content><author><name>Himanshu Roy</name></author><category term="metal3" /><category term="kubernetes" /><category term="cluster API" /><category term="metal3-dev-env" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Cluster API provider renaming</title><link href="https://metal3.io/blog/2020/03/05/CAPI_provider_renaming.html" rel="alternate" type="text/html" title="Cluster API provider renaming" /><published>2020-03-05T00:00:00-06:00</published><updated>2020-03-05T00:00:00-06:00</updated><id>https://metal3.io/blog/2020/03/05/CAPI_provider_renaming</id><content type="html" xml:base="https://metal3.io/blog/2020/03/05/CAPI_provider_renaming.html"><![CDATA[<h2 id="renaming-of-cluster-api-provider">Renaming of Cluster API provider</h2>

<div class="premonition info"><div class="fa fa-info-circle"></div><div class="content"><p class="header">Backwards compatibility for v1alpha3</p><p>There is no backwards compatibility between v1alpha3 and v1alpha2 releases of
the Cluster API provider for Metal3.</p>


</div></div>
<p>For the v1alpha3 release of Cluster API, the Metal3 provider was renamed from
<code class="language-plaintext highlighter-rouge">cluster-api-provider-baremetal</code> to <code class="language-plaintext highlighter-rouge">cluster-api-provider-metal3</code>. The Custom
Resource Definitions were also modified. This post dives into the changes.</p>

<h3 id="repository-renaming">Repository renaming</h3>

<p>From v1alpha3 onwards, the Cluster API provider will be developed in
<a href="https://github.com/metal3-io/cluster-api-provider-metal3">cluster-api-provider-metal3</a>.
The v1alpha1 and v1alpha2 content will remain in
<a href="https://github.com/metal3-io/cluster-api-provider-baremetal">cluster-api-provider-baremetal</a>.
This repository will be archived but kept for the integration in metal3-dev-env.</p>

<h3 id="custom-resource-definition-modifications">Custom Resource Definition modifications</h3>

<p>The kind of Custom Resource Definition (CRD) has been modified for the
following objects:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">BareMetalCluster</code> -&gt; <code class="language-plaintext highlighter-rouge">Metal3Cluster</code></li>
  <li><code class="language-plaintext highlighter-rouge">baremetalcluster</code> -&gt; <code class="language-plaintext highlighter-rouge">metal3cluster</code></li>
  <li><code class="language-plaintext highlighter-rouge">BareMetalMachine</code> -&gt; <code class="language-plaintext highlighter-rouge">Metal3Machine</code></li>
  <li><code class="language-plaintext highlighter-rouge">baremetalmachine</code> -&gt; <code class="language-plaintext highlighter-rouge">metal3machine</code></li>
  <li><code class="language-plaintext highlighter-rouge">BareMetalMachineTemplate</code> -&gt; <code class="language-plaintext highlighter-rouge">Metal3MachineTemplate</code></li>
  <li><code class="language-plaintext highlighter-rouge">baremetalmachinetemplate</code> -&gt; <code class="language-plaintext highlighter-rouge">metal3machinetemplate</code></li>
</ul>

<p>The custom resources deployed need to be modified accordingly.</p>

<h3 id="deployment-modifications">Deployment modifications</h3>

<p>The prefix of all deployed components for the Metal3 provider was modified
from <code class="language-plaintext highlighter-rouge">capbm-</code> to <code class="language-plaintext highlighter-rouge">capm3-</code>. The namespace in which the components are deployed by
default was modified from <code class="language-plaintext highlighter-rouge">capbm-system</code> to <code class="language-plaintext highlighter-rouge">capm3-system</code>.</p>]]></content><author><name>Maël Kimmerlin</name></author><category term="metal3" /><category term="baremetal" /><category term="cluster API" /><category term="provider" /><summary type="html"><![CDATA[Renaming of Cluster API provider]]></summary></entry></feed>